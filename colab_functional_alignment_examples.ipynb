{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0a84519-ed0f-4e35-9551-3cda0c174690",
   "metadata": {},
   "source": [
    "# Getting set up in Colab: \n",
    "First, we need to install the required packages for this tutorial into our colab session. This could take a few moments to install\n",
    "\n",
    "**If you are running this locally, you should use the other notebook -- not the one that starts with colab_** \n",
    "\n",
    "Step 1: Clone your fork of the github repository (`https://github.com/[YOUR USERNAME]/RepBiomrkr`) \n",
    "\n",
    "Step 2: Install required packages with pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba51ab-93fc-427f-a9a3-7df37202ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/[YOUR USERNAME]/RepBiomrkr\n",
    "! pip install -r RepBiomrkr/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e045ff-1312-4009-9a64-c36f15300aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import svm\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from sklearn.model_selection import KFold, LeaveOneOut\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from typing import List, Tuple, Optional, Union\n",
    "import random\n",
    "from brainiak.funcalign import srm\n",
    "import matplotlib.colors as mcolors\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from nilearn import datasets, surface\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "random.seed(4)\n",
    "np.random.seed(4)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83523910-fc01-42a5-8a89-a3de5fb672e5",
   "metadata": {},
   "source": [
    "# Including functions and classes here for use in colab \n",
    "if running locally, use the other functional_alignment_example notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa649109-c999-4411-9da3-e134554060fd",
   "metadata": {},
   "source": [
    "## Functions from `demo_data_helpers.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a982153d-8ff1-4d2d-9171-cd12c2b3af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sherlock_labels():\n",
    "    return np.load('RepBiomrkr/sample_data/sherlock_indoor_scene_labels.npy')\n",
    "    \n",
    "def load_rsfmri(n_subjects, save_to=\"RepBiomrkr/sample_data\", seed_region=16, verbose=1):\n",
    "    if save_to:\n",
    "        outpath = f'{save_to}/demo_rsfmri_data_{n_subjects}_subjects_seed_{seed_region}.pkl'\n",
    "        if os.path.exists(outpath):\n",
    "            with open(outpath, 'rb') as f:\n",
    "                loaded_data = pickle.load(f)\n",
    "            return loaded_data\n",
    "        \n",
    "    parcellated, region = process_rsfmri_demo_subjects_destrieux(n_subjects=n_subjects, region_id=seed_region, data_dir=None, verbose=verbose)\n",
    "    loaded_data = {'parcellated_all': parcellated, 'region_all':region}\n",
    "    if save_to:\n",
    "        with open(outpath, 'wb') as f:\n",
    "            pickle.dump(loaded_data, f)\n",
    "    return loaded_data\n",
    "\n",
    "def load_sherlock_movie():\n",
    "    return np.load(f'RepBiomrkr/sample_data/sherlock_movie_data_early_visual_roi.npy')\n",
    "\n",
    "def load_simulated_data_manifold_example():\n",
    "    loaded_data = {'input_data':np.load('RepBiomrkr/sample_data/input_data.npy'), \n",
    "                  'exogenous_data':np.load('RepBiomrkr/sample_data/exogenous_features.npy'),\n",
    "                  'scores':np.load('RepBiomrkr/sample_data/scores.npy')}\n",
    "    return loaded_data\n",
    "\n",
    "def compute_connectivity_matrix(m1, m2):\n",
    "    cnx = 1-cdist(m1, m2, 'correlation')\n",
    "    return cnx\n",
    "    \n",
    "\n",
    "def plot_3d_datasets(datasets, dataset_names=None, feature_names=None, title='',\n",
    "                     figsize=(6,4), marker='o', alpha=0.7, s=20, \n",
    "                     view_angles=(30, 45), save_path=None):\n",
    "    \"\"\"\n",
    "    Plot multiple datasets in a 3D scatterplot with different colors for each dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    datasets : list of numpy arrays\n",
    "        List containing N datasets, where each dataset is a numpy array of shape (S, 3)\n",
    "        with S samples and 3 features.\n",
    "    \n",
    "    dataset_names : list of str, optional\n",
    "        Names for each dataset to be shown in the legend. If None, datasets will be labeled\n",
    "        as \"Participant 1\", \"Participant 2\", etc.\n",
    "        \n",
    "    feature_names : list of str, optional\n",
    "        Names for the three features (x, y, z axes). If None, features will be labeled\n",
    "        as \"Feature 1\", \"Feature 2\", \"Feature 3\".\n",
    "    \n",
    "    title : str, optional\n",
    "        Title for the plot\n",
    "        \n",
    "    figsize : tuple, optional\n",
    "        Figure size as (width, height) in inches.\n",
    "    \n",
    "    marker : str, optional\n",
    "        Marker style for the scatter plot.\n",
    "    \n",
    "    alpha : float, optional\n",
    "        Transparency of the markers (0 to 1).\n",
    "    \n",
    "    s : int, optional\n",
    "        Marker size.\n",
    "    \n",
    "    view_angles : tuple, optional\n",
    "        Initial view angles (elevation, azimuth) in degrees.\n",
    "    \n",
    "    save_path : str, optional\n",
    "        If provided, saves the figure to the specified path.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    fig, ax : matplotlib figure and axis objects\n",
    "        The figure and axis objects for further customization if needed.\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    sns.set_context('paper')\n",
    "\n",
    "    if not isinstance(datasets, list):\n",
    "        raise TypeError(\"datasets must be a list of numpy arrays\")\n",
    "    \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        if dataset.shape[1] != 3:\n",
    "            raise ValueError(f\"Participant {i+1} has {dataset.shape[1]} features, expected 3 features\")\n",
    "    \n",
    "    # Set default names if not provided\n",
    "    if dataset_names is None:\n",
    "        dataset_names = [f\"Participant {i+1}\" for i in range(len(datasets))]\n",
    "    elif len(dataset_names) != len(datasets):\n",
    "        raise ValueError(\"Number of dataset names must match number of datasets\")\n",
    "    \n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"Feature {i+1}\" for i in range(3)]\n",
    "    elif len(feature_names) != 3:\n",
    "        raise ValueError(\"Must provide exactly 3 feature names\")\n",
    "    \n",
    "    # Create figure and 3D axis\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Get a color cycle for the datasets\n",
    "    colors = list(mcolors.TABLEAU_COLORS)  # Using Tableau colors\n",
    "    # If we have more datasets than colors, we'll cycle through the colors\n",
    "    if len(datasets) > len(colors):\n",
    "        colors = colors * (len(datasets) // len(colors) + 1)\n",
    "    \n",
    "    # Plot each dataset with a different color\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        ax.scatter(\n",
    "            dataset[:, 0], dataset[:, 1], dataset[:, 2],\n",
    "            c=colors[i], marker=marker, alpha=alpha, s=s,\n",
    "            label=dataset_names[i]\n",
    "        )\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel(feature_names[0])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_ylabel(feature_names[1])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_zlabel(feature_names[2])\n",
    "    ax.set_zticks([])\n",
    "    ax.set_title(f\"{title}\")\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend()\n",
    "    \n",
    "    # Set the viewing angle\n",
    "    ax.view_init(elev=view_angles[0], azim=view_angles[1])\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(False)\n",
    "    # Tight layout for better spacing\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure if path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "def plot_3d_trajectories(trajectories, trajectory_names=None, feature_names=None, title='',\n",
    "                         figsize=(6,4), linewidth=2, alpha=0.8, linestyle='-',\n",
    "                         markers=None, marker_size=20, marker_frequency=None,\n",
    "                         view_angles=(30, 45), save_path=None):\n",
    "    \"\"\"\n",
    "    Plot multiple trajectories (lines) in 3D space, each with a different color.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    trajectories : list of numpy arrays\n",
    "        List containing N trajectories, where each trajectory is a numpy array of shape (S, 3)\n",
    "        with S samples and 3 features, representing points along a path in 3D space.\n",
    "    \n",
    "    trajectory_names : list of str, optional\n",
    "        Names for each trajectory to be shown in the legend. If None, trajectories will be labeled\n",
    "        as \"Participant 1\", \"Participant 2\", etc.\n",
    "        \n",
    "    feature_names : list of str, optional\n",
    "        Names for the three features (x, y, z axes). If None, features will be labeled\n",
    "        as \"Feature 1\", \"Feature 2\", \"Feature 3\".\n",
    "    \n",
    "    title : str, optional\n",
    "        title of plot\n",
    "    \n",
    "    figsize : tuple, optional\n",
    "        Figure size as (width, height) in inches.\n",
    "    \n",
    "    linewidth : float, optional\n",
    "        Width of the trajectory lines.\n",
    "    \n",
    "    alpha : float, optional\n",
    "        Transparency of the lines (0 to 1).\n",
    "    \n",
    "    linestyle : str, optional\n",
    "        Style of the trajectory lines ('-', '--', '-.', ':', etc.).\n",
    "    \n",
    "    markers : str or list, optional\n",
    "        Marker style for points along the trajectory. If None, no markers are shown.\n",
    "        If a string, the same marker is used for all trajectories.\n",
    "        If a list, each trajectory can have a different marker.\n",
    "    \n",
    "    marker_size : int, optional\n",
    "        Size of markers if markers are shown.\n",
    "    \n",
    "    marker_frequency : int, optional\n",
    "        If provided, show markers every marker_frequency points along the trajectory.\n",
    "        If None and markers are provided, markers are shown at every point.\n",
    "    \n",
    "    view_angles : tuple, optional\n",
    "        Initial view angles (elevation, azimuth) in degrees.\n",
    "    \n",
    "    save_path : str, optional\n",
    "        If provided, saves the figure to the specified path.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    fig, ax : matplotlib figure and axis objects\n",
    "        The figure and axis objects for further customization if needed.\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if not isinstance(trajectories, list):\n",
    "        raise TypeError(\"trajectories must be a list of numpy arrays\")\n",
    "    \n",
    "    for i, trajectory in enumerate(trajectories):\n",
    "        if trajectory.shape[1] != 3:\n",
    "            raise ValueError(f\"Participant {i+1} has {trajectory.shape[1]} features, expected 3 features\")\n",
    "    \n",
    "    # Set default names if not provided\n",
    "    if trajectory_names is None:\n",
    "        trajectory_names = [f\"Participant {i+1}\" for i in range(len(trajectories))]\n",
    "    elif len(trajectory_names) != len(trajectories):\n",
    "        raise ValueError(\"Number of trajectory names must match number of trajectories\")\n",
    "    \n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"Feature {i+1}\" for i in range(3)]\n",
    "    elif len(feature_names) != 3:\n",
    "        raise ValueError(\"Must provide exactly 3 feature names\")\n",
    "    \n",
    "    # Handle markers\n",
    "    if markers is not None and not isinstance(markers, list):\n",
    "        markers = [markers] * len(trajectories)\n",
    "    elif markers is not None and len(markers) != len(trajectories):\n",
    "        markers = markers * (len(trajectories) // len(markers) + 1)\n",
    "        markers = markers[:len(trajectories)]\n",
    "    \n",
    "    # Create figure and 3D axis\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Get a color cycle for the trajectories\n",
    "    colors = list(mcolors.TABLEAU_COLORS)  # Using Tableau colors\n",
    "    # If we have more trajectories than colors, we'll cycle through the colors\n",
    "    if len(trajectories) > len(colors):\n",
    "        colors = colors * (len(trajectories) // len(colors) + 1)\n",
    "    \n",
    "    # Plot each trajectory with a different color\n",
    "    for i, trajectory in enumerate(trajectories):\n",
    "        # Plot the line\n",
    "        ax.plot(\n",
    "            trajectory[:, 0], trajectory[:, 1], trajectory[:, 2],\n",
    "            c=colors[i], linewidth=linewidth, alpha=alpha, linestyle=linestyle,\n",
    "            label=trajectory_names[i]\n",
    "        )\n",
    "        \n",
    "        # Add markers if requested\n",
    "        if markers is not None:\n",
    "            if marker_frequency is not None:\n",
    "                # Show markers at specified frequency\n",
    "                idx = np.arange(0, len(trajectory), marker_frequency)\n",
    "                marker_data = trajectory[idx]\n",
    "            else:\n",
    "                # Show markers at every point\n",
    "                marker_data = trajectory\n",
    "                \n",
    "            ax.scatter(\n",
    "                marker_data[:, 0], marker_data[:, 1], marker_data[:, 2],\n",
    "                c=colors[i], marker=markers[i], s=marker_size\n",
    "            )\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel(feature_names[0])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_ylabel(feature_names[1])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_zlabel(feature_names[2])\n",
    "    ax.set_zticks([])\n",
    "    ax.set_title(f\"{title}\")\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend()\n",
    "    \n",
    "    # Set the viewing angle\n",
    "    ax.view_init(elev=view_angles[0], azim=view_angles[1])\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(False)\n",
    "    # Tight layout for better spacing\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure if path is provided\n",
    "    if save_path: plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac2aaa4-0f36-4fef-8e7c-9732a5fafba7",
   "metadata": {},
   "source": [
    "## Functions from `load_demo_rsfmri.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666b3612-2d4f-4b23-b645-81b323af1954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_subjects_data(n_subjects=10, data_dir=None, verbose=0):\n",
    "    \"\"\"\n",
    "    Download surface data for n_subjects from the NKI enhanced dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_subjects : int, default=10\n",
    "        Number of subjects to download.\n",
    "    data_dir : str, optional\n",
    "        Directory where data should be downloaded.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list of dicts\n",
    "        Each dict contains paths to the surface data of a subject.\n",
    "    \"\"\"\n",
    "    if verbose: print(f\"Downloading data for {n_subjects} subjects...\")\n",
    "    \n",
    "    # Fetch data for all subjects\n",
    "    nki_data = datasets.fetch_surf_nki_enhanced(n_subjects=n_subjects, data_dir=data_dir)\n",
    "    \n",
    "    # Extract only the first n_subjects\n",
    "    subjects_data = []\n",
    "    for i in range(n_subjects):\n",
    "        subject_data = {\n",
    "            'func_left': np.nan_to_num(nki_data.func_left[i]),\n",
    "            'func_right': np.nan_to_num(nki_data.func_right[i])\n",
    "        }\n",
    "        subjects_data.append(subject_data)\n",
    "    \n",
    "    if verbose: print(f\"Downloaded {len(subjects_data)} subjects' data.\")\n",
    "    return subjects_data\n",
    "\n",
    "def fetch_destrieux_atlas(data_dir=None):\n",
    "    \"\"\"\n",
    "    Fetch the Destrieux atlas.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir : str, optional\n",
    "        Directory where data should be downloaded.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing the atlas data.\n",
    "    \"\"\"\n",
    "    print(\"Fetching Destrieux atlas...\")\n",
    "    \n",
    "    # Fetch the fsaverage5 surface and Destrieux parcellation\n",
    "    fsaverage = datasets.fetch_surf_fsaverage(mesh='fsaverage5', data_dir=data_dir)\n",
    "    \n",
    "    # Load the Destrieux parcellation\n",
    "    destrieux = datasets.fetch_atlas_surf_destrieux(data_dir=data_dir)\n",
    "    \n",
    "    atlas = {\n",
    "        'parcellation_left': destrieux['map_left'],\n",
    "        'parcellation_right': destrieux['map_right'],\n",
    "        'labels': destrieux['labels'],\n",
    "        'mesh_left': fsaverage.pial_left,\n",
    "        'mesh_right': fsaverage.pial_right\n",
    "    }\n",
    "    \n",
    "    print(f\"Fetched Destrieux atlas with {len(atlas['labels'])} regions.\")\n",
    "    return atlas\n",
    "\n",
    "def extract_surface_data(subject_data):\n",
    "    \"\"\"\n",
    "    Extract surface functional data from a subject.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subject_data : dict\n",
    "        Dictionary containing paths to subject's data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (left_data, right_data) numpy arrays of shape (n_vertices, n_timepoints)\n",
    "    \"\"\"\n",
    "    # Load left and right hemisphere data\n",
    "    left_data = surface.load_surf_data(subject_data['func_left'])\n",
    "    right_data = surface.load_surf_data(subject_data['func_right'])\n",
    "    \n",
    "    # Transpose to shape (n_vertices, n_timepoints)\n",
    "    if left_data.shape[0] < left_data.shape[1]:\n",
    "        left_data = left_data.T\n",
    "    if right_data.shape[0] < right_data.shape[1]:\n",
    "        right_data = right_data.T\n",
    "    \n",
    "    return left_data, right_data\n",
    "\n",
    "def load_atlas_labels(atlas):\n",
    "    \"\"\"\n",
    "    Load atlas labels for left and right hemispheres.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    atlas : dict\n",
    "        Atlas data returned.\n",
    "    mesh_left : str\n",
    "        Path to left hemisphere mesh.\n",
    "    mesh_right : str\n",
    "        Path to right hemisphere mesh.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (left_labels, right_labels) containing label arrays for each hemisphere.\n",
    "    \"\"\"\n",
    "    # Load the fsaverage5 surface\n",
    "    fsaverage = datasets.fetch_surf_fsaverage()\n",
    "    \n",
    "    # Map the atlas from MNI volume to fsaverage5 surface\n",
    "    labels_left = surface.vol_to_surf(\n",
    "        atlas.maps, fsaverage.pial_left\n",
    "    )\n",
    "    labels_right = surface.vol_to_surf(\n",
    "        atlas.maps, fsaverage.pial_right\n",
    "    )\n",
    "    \n",
    "    return labels_left, labels_right\n",
    "\n",
    "def parcellate_hemisphere_data(hemisphere_data, hemisphere_labels):\n",
    "    \"\"\"\n",
    "    Parcellate data for a single hemisphere using the provided labels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hemisphere_data : numpy.ndarray\n",
    "        Hemisphere data with shape (n_vertices, n_timepoints).\n",
    "    hemisphere_labels : numpy.ndarray\n",
    "        Labels array for the hemisphere.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (parcellated_data, unique_rois) where:\n",
    "        - parcellated_data has shape (n_timepoints, n_rois)\n",
    "        - unique_rois is a list of unique region IDs\n",
    "    \"\"\"\n",
    "    # Get number of timepoints\n",
    "    n_timepoints = hemisphere_data.shape[1]\n",
    "    \n",
    "    # Get unique ROIs (exclude 0 which is typically background)\n",
    "    unique_rois = np.unique(hemisphere_labels)\n",
    "    unique_rois = unique_rois[unique_rois != 0]\n",
    "    n_rois = len(unique_rois)\n",
    "    \n",
    "    # Initialize parcellated data\n",
    "    parcellated_data = np.zeros((n_timepoints, n_rois))\n",
    "    \n",
    "    # Maps from ROI value to index in parcellated_data\n",
    "    roi_to_idx = {roi: i for i, roi in enumerate(unique_rois)}\n",
    "    \n",
    "    # Parcellate hemisphere\n",
    "    for roi in unique_rois:\n",
    "        roi_mask = hemisphere_labels == roi\n",
    "        if np.any(roi_mask):\n",
    "            roi_data = hemisphere_data[roi_mask, :]\n",
    "            idx = roi_to_idx[roi]\n",
    "            parcellated_data[:, idx] = np.mean(roi_data, axis=0)\n",
    "    \n",
    "    return parcellated_data\n",
    "\n",
    "def parcellate_data_separate_hemispheres(subject_data, atlas):\n",
    "    \"\"\"\n",
    "    Parcellate subject's data using the Destrieux atlas, keeping hemispheres separate.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subject_data : dict\n",
    "        Dictionary containing paths to subject's data.\n",
    "    atlas : dict\n",
    "        Atlas data returned by fetch_destrieux_atlas.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing parcellated data and ROI information for each hemisphere.\n",
    "    \"\"\"\n",
    "    # Extract surface data\n",
    "    left_data, right_data = extract_surface_data(subject_data)\n",
    "    \n",
    "    # Get labels from left and right hemispheres\n",
    "    labels_left = np.array(atlas['parcellation_left'])\n",
    "    labels_right = np.array(atlas['parcellation_right'])\n",
    "    \n",
    "    # Parcellate left hemisphere\n",
    "    parcellated_left = parcellate_hemisphere_data(left_data, labels_left)\n",
    "    \n",
    "    # Parcellate right hemisphere\n",
    "    parcellated_right = parcellate_hemisphere_data(right_data, labels_right)\n",
    "    \n",
    "    return parcellated_left, parcellated_right\n",
    "    \n",
    "\n",
    "\n",
    "def extract_region_data_separate_hemispheres(subject_data, atlas, region_id):\n",
    "    \"\"\"\n",
    "    Extract multivariate data from a specific region, keeping hemispheres separate.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subject_data : dict\n",
    "        Dictionary containing paths to subject's data.\n",
    "    atlas : dict\n",
    "        Atlas data returned by fetch_destrieux_atlas.\n",
    "    region_id : int, default=16\n",
    "        Region ID to extract.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing region data for each hemisphere.\n",
    "    \"\"\"\n",
    "    # Extract surface data\n",
    "    left_data, right_data = extract_surface_data(subject_data)\n",
    "    \n",
    "    # Get labels from left and right hemispheres\n",
    "    labels_left = np.array(atlas['parcellation_left'])\n",
    "    labels_right = np.array(atlas['parcellation_right'])\n",
    "    \n",
    "    # Extract vertices from the specified region for each hemisphere\n",
    "    region_mask_left = left_data[labels_left == region_id,:]\n",
    "    region_mask_right = right_data[labels_right == region_id,:]\n",
    "    \n",
    "    return region_mask_left.T, region_mask_right.T\n",
    "    \n",
    "def get_region_name(atlas, region_id):\n",
    "    \"\"\"\n",
    "    Get the name of a region by its ID.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    atlas : dict\n",
    "        Atlas data returned by fetch_destrieux_atlas.\n",
    "    region_id : int\n",
    "        Region ID.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Region name.\n",
    "    \"\"\"\n",
    "    label = atlas['labels'][region_id+1].decode() # account for background\n",
    "    return label\n",
    "\n",
    "def process_rsfmri_demo_subjects_destrieux(n_subjects=10, region_id=16, data_dir=None, verbose=0):\n",
    "    \"\"\"\n",
    "    Process data for multiple subjects using the Destrieux atlas,\n",
    "    keeping hemispheres separate.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_subjects : int, default=10\n",
    "        Number of subjects to process.\n",
    "    region_id : int, default=16\n",
    "        Region ID to extract data from.\n",
    "    data_dir : str, optional\n",
    "        Directory where data should be downloaded.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing processed data for each hemisphere.\n",
    "    \"\"\"\n",
    "    # Download data for subjects\n",
    "    subjects_data = download_subjects_data(n_subjects=n_subjects, data_dir=data_dir)\n",
    "    \n",
    "    # Fetch Destrieux atlas\n",
    "    atlas = fetch_destrieux_atlas(data_dir=data_dir)\n",
    "    \n",
    "    # Check if the specified region exists in the atlas\n",
    "    region_name = get_region_name(atlas, region_id)\n",
    "    print(f\"Target region: {region_name}\")\n",
    "    \n",
    "    # Initialize dictionaries to store results\n",
    "    parcellated_all, region_all = [],[]\n",
    "    \n",
    "    # Process each subject\n",
    "    for i, subject_data in enumerate(tqdm(subjects_data, desc=\"Processing subjects\")):\n",
    "        # Parcellate data\n",
    "        parcellated_left, parcellated_right = parcellate_data_separate_hemispheres(subject_data, atlas)\n",
    "        \n",
    "        # Extract region data\n",
    "        region_data_left, region_data_right = extract_region_data_separate_hemispheres(subject_data, atlas, region_id=region_id)\n",
    "        if verbose: print(f'subject {i+1} parcellated data of shape: {parcellated_left.shape}, {parcellated_right.shape}')\n",
    "        if verbose: print(f'subject {i+1} parcellated data of shape: {region_data_left.shape}, {region_data_right.shape}')\n",
    "        # Normalize data\n",
    "        parcellated_left_norm = np.nan_to_num(scipy.stats.zscore(parcellated_left, axis=0))\n",
    "        parcellated_right_norm = np.nan_to_num(scipy.stats.zscore(parcellated_right, axis=0))\n",
    "        region_left_norm = np.nan_to_num(scipy.stats.zscore(region_data_left, axis=0))\n",
    "        region_right_norm = np.nan_to_num(scipy.stats.zscore(region_data_right, axis=0))\n",
    "        \n",
    "        # Stack left and right together\n",
    "        parcellated_bh = np.hstack([parcellated_left_norm, parcellated_right_norm])\n",
    "        region_bh = np.hstack([region_left_norm, region_right_norm])\n",
    "        if verbose: print(f'after stacking: {parcellated_bh.shape}, {region_bh.shape}')\n",
    "        parcellated_all.append(parcellated_bh)\n",
    "        region_all.append(region_bh)\n",
    "        \n",
    "    return parcellated_all, region_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102322e7-9a85-43b2-8baa-fd9ac57f919f",
   "metadata": {},
   "source": [
    "## Class from `hyperalignment.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7817e0-6201-4bea-b4c1-9573f82eb367",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperalignment(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Hyperalignment transformer that follows scikit-learn's API conventions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_iter : int, default=10\n",
    "        Maximum number of iterations for hyperalignment optimization.\n",
    "    \n",
    "    scaling : bool, default=True\n",
    "        Whether to apply scaling during Procrustes transformations.\n",
    "    \n",
    "    convergence_threshold : float, default=0.1\n",
    "        Early stopping threshold for mean disparity between aligned datasets.\n",
    "    \n",
    "    verbose : bool, default=False\n",
    "        If True, print progress during iterations.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    transformations : list\n",
    "        List of transformation matrices for each dataset.\n",
    "    \n",
    "    template : ndarray\n",
    "        Common template space derived from training data.\n",
    "    \n",
    "    n_features_in : int\n",
    "        Number of features in the input datasets.\n",
    "        \n",
    "    n_features_target: int\n",
    "    \n",
    "    aligned_data : list\n",
    "        List of datasets aligned to the common space.\n",
    "    \n",
    "    n_iterations_ : int\n",
    "        Number of iterations performed during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_iterations=10, scaling=True, center_only=True, convergence_threshold=0.001, n_features_target=0, verbose=False): \n",
    "        self.n_iterations = n_iterations\n",
    "        self.scaling = scaling\n",
    "        self.convergence_threshold = convergence_threshold\n",
    "        self.verbose = verbose\n",
    "        self.training_data = None\n",
    "        self.n_features_target = n_features_target\n",
    "        self.center_only = center_only\n",
    "    \n",
    "    def _normalize_data(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Normalize data by centering and scaling.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            Input data matrix.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            Normalized data matrix.\n",
    "        \"\"\"\n",
    "        X -= np.mean(X, axis=0)\n",
    "        norm = np.linalg.norm(X)\n",
    "        if norm == 0: \n",
    "            raise ValueError(\"input matrix must have >1 unique points\")\n",
    "        if self.center_only:\n",
    "            return X\n",
    "        return  X / norm\n",
    "    \n",
    "    def _disparity(self, M1: np.ndarray, M2: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate squared Euclidean distance between two matrices.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        M1 : ndarray\n",
    "            First data matrix.\n",
    "        M2 : ndarray\n",
    "            Second data matrix.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Sum of squared differences between matrices.\n",
    "        \"\"\"\n",
    "        return np.sum(np.square(M1 - M2))\n",
    "    \n",
    "    def _procrustes(self, source: np.ndarray, target: np.ndarray, \n",
    "                   reduction: bool = False, scaling: Optional[bool] = None) -> Tuple:\n",
    "        \"\"\"\n",
    "        Perform Procrustes transformation to align source to target.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        source : ndarray\n",
    "            Source data matrix to be aligned.\n",
    "        target : ndarray\n",
    "            Target data matrix to align to.\n",
    "        reduction : bool, default=False\n",
    "            If True, reduce dimensions if source and target have different numbers of features.\n",
    "        scaling : bool, optional\n",
    "            Whether to apply scaling. If None, use self.scaling.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            (R, aligned_source, disparity, scale)\n",
    "            - R: Transformation matrix\n",
    "            - aligned_source: Transformed source data\n",
    "            - disparity: Measure of difference between aligned source and target\n",
    "            - scale: Scaling factor applied\n",
    "        \"\"\"\n",
    "        if scaling is None: scaling = self.scaling\n",
    "            \n",
    "        n_features_source = source.shape[-1]\n",
    "        n_features_target = target.shape[-1]\n",
    "        \n",
    "        # normalize data\n",
    "        target_norm = self._normalize_data(target)\n",
    "        source_norm = self._normalize_data(source)\n",
    "        scale = 1.0\n",
    "        \n",
    "        # Account for lower dimensions\n",
    "        if n_features_source > n_features_target:\n",
    "            reduction = True\n",
    "            temp = np.zeros_like(source)\n",
    "            temp[:, :n_features_target] = target_norm\n",
    "            target_norm = temp\n",
    "        \n",
    "        # calculate optimal rotation\n",
    "        U, s, vt = np.linalg.svd((target_norm.T @ np.conjugate(source_norm)).T)\n",
    "        R = U @ vt\n",
    "        if reduction:\n",
    "            # select only relevant dimensions of the transformation\n",
    "            R = R[:n_features_source, :n_features_target]\n",
    "            target_norm = target_norm[:,:n_features_target]\n",
    "            s=s[:n_features_target]\n",
    "            \n",
    "        \n",
    "        if scaling:\n",
    "            # scale by the sum of the singular values\n",
    "            scale = s.sum()\n",
    "        \n",
    "        R *= scale\n",
    "        \n",
    "        # Apply transformation\n",
    "        new_source = source_norm @ R\n",
    "        \n",
    "        # Check if overflow\n",
    "        if np.isinf(np.linalg.norm(new_source)):\n",
    "            return\n",
    "        \n",
    "        # Measure dissimilarity\n",
    "        disp = self._disparity(target_norm, new_source)\n",
    "        \n",
    "        return R, new_source, disp, scale\n",
    "    \n",
    "    def _create_initial_template(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create initial template from a list of datasets.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        datasets : list of ndarray\n",
    "            List of datasets to create a template from.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            Initial template for alignment.\n",
    "        \"\"\"\n",
    "        # Initialize to the first just for now\n",
    "        for i, x in enumerate(self.training_data):\n",
    "            if i == 0:\n",
    "                # use first data as template\n",
    "                template = np.copy(x)\n",
    "            else:\n",
    "                try:\n",
    "                    _, aligned, _, _ = self._procrustes(template / i, x)\n",
    "                except:\n",
    "                    return np.array([0])\n",
    "                template += aligned\n",
    "                \n",
    "        template /= len(self.training_data)\n",
    "        return template\n",
    "    \n",
    "    def _create_level2_template(self, datasets: List[np.ndarray], \n",
    "                               template_prev: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create improved template by aligning datasets to previous template.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        datasets : list of ndarray\n",
    "            List of datasets to align.\n",
    "        template_prev : ndarray\n",
    "            Previous template to align to.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            Improved template for alignment.\n",
    "        \"\"\"\n",
    "        new_template = np.zeros_like(template_prev)\n",
    "        \n",
    "        for x in datasets:\n",
    "            _, aligned, _, _ = self._procrustes(template_prev, x)\n",
    "            new_template += aligned\n",
    "        \n",
    "        new_template /= len(datasets)\n",
    "        return new_template\n",
    "    \n",
    "    def fit(self, X: List[np.ndarray], y=None) -> 'Hyperalignment':\n",
    "        \"\"\"\n",
    "        Fit the hyperalignment model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : list of ndarray\n",
    "            List of datasets to align, where each dataset is a 2D array of shape\n",
    "            (n_samples, n_features). All datasets should have the same number of features and samples.\n",
    "        y : ignored, present for API consistency.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : Hyperalignment\n",
    "            Returns fitted model.\n",
    "        \"\"\"\n",
    "        if not (isinstance(X, np.ndarray) and X.ndim == 3 or isinstance(X, list)):\n",
    "            raise ValueError(\"Input should be a 3D array or list of 2D arrays\")\n",
    "        \n",
    "        if len(X) < 2:\n",
    "            raise ValueError(\"At least two datasets are required for hyperalignment\")\n",
    "        \n",
    "        # Store number of features\n",
    "        self.n_features_in = X[0].shape[1]\n",
    "        self.training_data = X\n",
    "        reduction = False\n",
    "        if self.n_features_target == 0: \n",
    "            self.n_features_target = self.n_features_in\n",
    "           \n",
    "        if self.n_features_target < self.n_features_in:\n",
    "            reduction = True\n",
    "            \n",
    "            \n",
    "        # Check all datasets have the same number of features\n",
    "        if not all(x.shape[1] == self.n_features_in for x in X):\n",
    "            raise ValueError(\"All datasets must have the same number of features\")\n",
    "        \n",
    "        # Create the initial template\n",
    "        template = self._create_initial_template()\n",
    "        if template.ndim == 1:\n",
    "            if self.verbose: print(f\"Normalizing data...\")\n",
    "            self.center_only = False\n",
    "            template = self._create_initial_template()\n",
    "\n",
    "         # Initialize as the training data\n",
    "        aligned = X.copy()\n",
    "        \n",
    "        # Initialize as empty\n",
    "        transformations = None\n",
    "        # Go through iterations\n",
    "        for i in range(self.n_iterations):\n",
    "            template = self._create_level2_template(aligned, template)\n",
    "            # template = self._normalize_data(template)\n",
    "            scales, disps, aligned1, transformations1 = [], [], [], []\n",
    "            for src in aligned:\n",
    "                R, N, d, s = self._procrustes(src, template, scaling=self.scaling)\n",
    "                scales.append(s)\n",
    "                disps.append(d)\n",
    "                aligned1.append(N)\n",
    "                transformations1.append(R)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f'Iteration {i+1}, disparity={np.mean(disps):.4f}')\n",
    "            \n",
    "            if np.mean(disps) < 0.1:\n",
    "                if self.verbose: print(f'breaking on iter {i}, disp={np.mean(disps):04f}')\n",
    "                break\n",
    "                \n",
    "            aligned = aligned1\n",
    "            transformations = transformations1\n",
    "        \n",
    "        # Now take the training data and align it to the final template\n",
    "        final_template = template\n",
    "        \n",
    "        if reduction:\n",
    "            # Reduce the dimensionality of the template\n",
    "            final_template, components = self._reduce_dimensionality(template)\n",
    "            \n",
    "        final_aligned, final_transformations, final_disparity, final_scaling = [], [], [], []\n",
    "        for src in X:\n",
    "            trans, aligned_ds, disp, s = self._procrustes(src, final_template, scaling=self.scaling, reduction=reduction)\n",
    "            final_scaling.append(s)\n",
    "            final_aligned.append(aligned_ds)\n",
    "            final_transformations.append(trans)\n",
    "            final_disparity.append(disp)\n",
    "        if self.verbose: print(f'final average disparity: {np.mean(final_disparity):.4f}')\n",
    "        \n",
    "        self.n_iterations = i + 1\n",
    "        self.transformations = final_transformations\n",
    "        self.template = final_template\n",
    "        self.aligned_data = final_aligned    \n",
    "        self.disparity = final_disparity\n",
    "        self.scaling_factors=final_scaling\n",
    "        return self\n",
    "    \n",
    "    def _reduce_dimensionality(self, template):\n",
    "        \"\"\"\n",
    "        Apply PCA to transform data into a lower dimensional space, using SVD for numerical stability.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            Input data matrix of shape (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        X_transformed : ndarray\n",
    "            Data projected into principal component space, shape (n_samples, n_components)\n",
    "        components : ndarray\n",
    "            Principal components (eigenvectors), shape (n_features, n_components)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Center the data\n",
    "        if self.verbose:\n",
    "            print(f'Reducing dimensionality of template')\n",
    "        template -= np.mean(template, axis=0)\n",
    "        n_components, n_samples, n_features = self.n_features_target, template.shape[0], template.shape[1]\n",
    "        # Compute the SVD of the template\n",
    "        # U: left singular vectors, shape (n_samples, n_samples)\n",
    "        # s: singular values, shape (min(n_samples, n_features),)\n",
    "        # Vh: right singular vectors, shape (n_features, n_features)\n",
    "        U, s, Vh = np.linalg.svd(template, full_matrices=False)\n",
    "        # Calculate explained variance\n",
    "        # The singular values are related to eigenvalues of the covariance matrix\n",
    "        explained_variance = (s ** 2) / (n_samples - 1)\n",
    "        explained_variance_ratio = explained_variance / explained_variance.sum()\n",
    "        if self.verbose:\n",
    "            print(f'running PCA over the template; first {n_components} components explain {np.sum(explained_variance_ratio[:n_components])*100:.4f} percent variance')\n",
    "        # Get the principal components (eigenvectors)\n",
    "        # Vh contains the right singular vectors (PCs)\n",
    "        components = Vh[:n_components].T  # Transpose to get shape (n_features, n_components)\n",
    "        # Transform the data\n",
    "        template_transformed = template @ components\n",
    "        return template_transformed, components\n",
    "    \n",
    "    def transform(self, X: Union[np.ndarray, List[np.ndarray]]) -> Union[np.ndarray, List[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Applies the transformation matrices learned during fitting to new datasets. \n",
    "        Assumes that the order of new datasets matches the order of the transformation matrices.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : list of ndarray\n",
    "            List of datasets to align.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list of ndarray\n",
    "            Transformation matrices for each dataset.\n",
    "        \"\"\"\n",
    "        transformations = self.get_transformations()\n",
    "        if not len(X) == len(transformations):\n",
    "            raise ValueError(f\"Length of new datasets {len(X)} not equal to number of transformations; did you mean to compute a new transformation?\")\n",
    "        \n",
    "        transformed_data = []\n",
    "        for x, T in zip(X, transformations):\n",
    "            # normalize the data\n",
    "            x = self._normalize_data(x)\n",
    "            transformed = x @ T\n",
    "            transformed_data.append(transformed)\n",
    "        return transformed_data\n",
    "        \n",
    "    \n",
    "    def fit_transform(self, X: List[np.ndarray], y=None) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Fit the model and transform the training data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : list of ndarray\n",
    "            List of datasets to align.\n",
    "        y : ignored\n",
    "            Not used, present for API consistency.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        list of ndarray\n",
    "            Aligned versions of the input datasets.\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.aligned_data\n",
    "    \n",
    "    def get_transformations(self) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Get the transformation matrices learned during fitting.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list of ndarray\n",
    "            Transformation matrices for each dataset.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'transformations'):\n",
    "            raise ValueError(\"Model not fitted yet. Call 'fit' first.\")\n",
    "        return self.transformations\n",
    "    \n",
    "    def transform_new_dataset(self, X) -> Tuple:\n",
    "        \"\"\"\n",
    "        Aligns a new dataset, assumed to have the same shape as the training datasets, to the fitted template \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            Dataset to align.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple: (Aligned data, transformation)\n",
    "        \"\"\"\n",
    "        if not X.shape != self.template.shape: \n",
    "            raise ValueError(f\"Shape of new dataset {X.shape} does not match the template {self.template.shape}\")\n",
    "        \n",
    "        R, aligned_ds, _, _ = self._procrustes(X, self.template)\n",
    "        return (aligned_ds, R)\n",
    "    \n",
    "    def get_template(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the common template space.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            Template representing the common space.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'template'):\n",
    "            raise ValueError(\"Model not fitted yet. Call 'fit' first.\")\n",
    "        return self.template\n",
    "    \n",
    "    def _run_isc(self, X: List[np.ndarray]):\n",
    "        '''\n",
    "        Intersubject correlation analysis: \n",
    "        compares each subject's timeseries with the average of all other subjects' timeseries\n",
    "        Measure of \"synchrony\" over time across brains, which should be improved by functional alignment\n",
    "        '''\n",
    "        if len(np.shape(X)) == 2:\n",
    "            X = X[:,:,np.newaxis]\n",
    "        results = np.zeros((len(X), X[0].shape[-1]))\n",
    "        for i in range(len(X)):\n",
    "            test = X[i]\n",
    "            tr_idx = np.setdiff1d(np.arange(len(X)),i)\n",
    "            train = np.mean(np.array([X[j] for j in tr_idx]), axis=0)\n",
    "            # get correlation at each feature\n",
    "            cmat = cdist(test.T, train.T, 'correlation')\n",
    "            corrs = [1-cmat[i,i] for i in range(cmat.shape[0])]\n",
    "            results[i]= corrs\n",
    "        return results\n",
    "    \n",
    "    def evaluate_isc(self):\n",
    "        '''\n",
    "        built in function to return ISC scores for the training datasets, pre and post hyperalignment\n",
    "        '''\n",
    "        \n",
    "        original_isc = self._run_isc(self.training_data)\n",
    "        aligned_isc = self._run_isc(self.aligned_data)\n",
    "        if self.verbose: \n",
    "            print(f\"Pre-alignment ISC: {np.mean(original_isc):.4f}\\nPost-alignment ISC: {np.mean(aligned_isc):.4f}\")\n",
    "        return {\"Pre-alignment\":original_isc, \"Post-alignment\":aligned_isc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c076863-bbf6-4639-8b3a-93fcb1fe40ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Class from `simulate_correlated_data.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e94b2a8-52eb-4d01-be6f-dfa00ba1e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulatedCorrelatedData:\n",
    "    \"\"\"\n",
    "    A class for generating and analyzing datasets with controlled correlation structure.\n",
    "    \n",
    "    This class can create multiple datasets with specified intra- and inter-dataset correlations,\n",
    "    based on latent features that are projected to measured features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_datasets=3, n_samples=500, n_features_measured=10, \n",
    "                 n_features_latent=0, noise_sigma=0.1, inter_ds_corr=0.3, \n",
    "                 intra_ds_corr=0.7, verbose=0):\n",
    "        \"\"\"\n",
    "        Initialize the SimulatedCorrelatedData object with parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_datasets : int\n",
    "            Number of datasets to generate\n",
    "        n_samples : int\n",
    "            Number of samples in each dataset\n",
    "        n_features_measured : int\n",
    "            Number of measured features in each dataset\n",
    "        n_features_latent : int\n",
    "            Number of latent features that generate the measured features\n",
    "        noise_sigma : float\n",
    "            Standard deviation of the noise\n",
    "        inter_ds_corr : float\n",
    "            Target average correlation between datasets\n",
    "        intra_ds_corr : float\n",
    "            Target average correlation between features within a dataset\n",
    "        verbose : int\n",
    "            Level of verbosity (0=quiet, 1=verbose)\n",
    "        \"\"\"\n",
    "        self.n_datasets = n_datasets\n",
    "        self.n_samples = n_samples\n",
    "        self.n_features_measured = n_features_measured\n",
    "        self.noise_sigma = noise_sigma\n",
    "        self.inter_ds_corr = inter_ds_corr\n",
    "        self.intra_ds_corr = intra_ds_corr\n",
    "        self.verbose = verbose\n",
    "        if n_features_latent == 0:\n",
    "            self.n_features_latent = n_features_measured\n",
    "        else:\n",
    "            self.n_features_latent = n_features_latent\n",
    "\n",
    "        # Datasets will be stored here after generation\n",
    "        self.datasets = None\n",
    "        self.latent_features = None\n",
    "        \n",
    "        # Correlation statistics\n",
    "        self.avg_intra_latent_corr = None\n",
    "        self.avg_inter_latent_corr = None\n",
    "        self.avg_intra_measured_corr = None\n",
    "        self.avg_inter_measured_corr = None\n",
    "        self.intra_measured_corrs = None\n",
    "        self.inter_measured_corrs = None\n",
    "    \n",
    "    def _compute_intra_dataset_correlation(self, data_matrix):\n",
    "        \"\"\"\n",
    "        Calculate the average pairwise correlation between features within a dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_matrix : numpy.ndarray\n",
    "            Data matrix of shape [n_samples, n_features]\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Average pairwise correlation between features\n",
    "        \"\"\"\n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = np.corrcoef(data_matrix.T)\n",
    "        \n",
    "        # Extract off-diagonal elements (exclude self-correlations)\n",
    "        off_diagonal_mask = ~np.eye(corr_matrix.shape[0], dtype=bool)\n",
    "        off_diagonal_corrs = corr_matrix[off_diagonal_mask]\n",
    "        \n",
    "        # Return average correlation\n",
    "        return np.mean(off_diagonal_corrs)\n",
    "    \n",
    "    def _compute_average_intra_dataset_correlations(self, datasets):\n",
    "        \"\"\"\n",
    "        Calculate the average intra-dataset correlation across multiple datasets.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        datasets : list of numpy.ndarray\n",
    "            List of data matrices, each of shape [n_samples, n_features]\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Average intra-dataset correlation across all datasets\n",
    "        list of float\n",
    "            Individual intra-dataset correlations for each dataset\n",
    "        \"\"\"\n",
    "        intra_ds_corrs = []\n",
    "        \n",
    "        for dataset in datasets:\n",
    "            intra_ds_corrs.append(self._compute_intra_dataset_correlation(dataset))\n",
    "        \n",
    "        return np.mean(intra_ds_corrs), intra_ds_corrs\n",
    "    \n",
    "    def _compute_inter_dataset_correlation(self, dataset1, dataset2):\n",
    "        \"\"\"\n",
    "        Calculate the average correlation between features from two different datasets.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dataset1 : numpy.ndarray\n",
    "            First data matrix of shape [n_samples, n_features1]\n",
    "        dataset2 : numpy.ndarray\n",
    "            Second data matrix of shape [n_samples, n_features2]\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Average correlation between features from different datasets\n",
    "        \"\"\"\n",
    "        n_features1 = dataset1.shape[1]\n",
    "        n_features2 = dataset2.shape[1]\n",
    "        \n",
    "        # Compute correlations between each pair of features\n",
    "        correlations = []\n",
    "        for i in range(n_features1):\n",
    "            for j in range(n_features2):\n",
    "                corr = np.corrcoef(dataset1[:, i], dataset2[:, j])[0, 1]\n",
    "                correlations.append(corr)\n",
    "        \n",
    "        return np.mean(correlations)\n",
    "    \n",
    "    def _compute_average_inter_dataset_correlations(self, datasets):\n",
    "        \"\"\"\n",
    "        Calculate the average inter-dataset correlation across multiple datasets.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        datasets : list of numpy.ndarray\n",
    "            List of data matrices, each of shape [n_samples, n_features]\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Average inter-dataset correlation across all dataset pairs\n",
    "        list of float\n",
    "            Individual inter-dataset correlations for each dataset pair\n",
    "        \"\"\"\n",
    "        n_datasets = len(datasets)\n",
    "        inter_ds_corrs = []\n",
    "        \n",
    "        for i in range(n_datasets):\n",
    "            for j in range(i+1, n_datasets):\n",
    "                inter_ds_corrs.append(self._compute_inter_dataset_correlation(datasets[i], datasets[j]))\n",
    "        \n",
    "        return np.mean(inter_ds_corrs), inter_ds_corrs\n",
    "    \n",
    "    def _compute_latent_correlations(self):\n",
    "        \"\"\"\n",
    "        Calculate the correlation structure of latent features.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (avg_intra_ds_corr, avg_inter_ds_corr)\n",
    "        \"\"\"\n",
    "        if self.latent_features is None:\n",
    "            raise ValueError(\"Latent features have not been generated yet. Call generate() first.\")\n",
    "            \n",
    "        # Calculate correlation matrix of latent features\n",
    "        actual_corr = np.corrcoef(self.latent_features.T)\n",
    "        \n",
    "        # Compute average intra-dataset correlation\n",
    "        intra_ds_corrs = []\n",
    "        for ds in range(self.n_datasets):\n",
    "            start_idx = ds * self.n_features_latent\n",
    "            end_idx = start_idx + self.n_features_latent\n",
    "            ds_corr = actual_corr[start_idx:end_idx, start_idx:end_idx]\n",
    "            \n",
    "            # Exclude the diagonal (self-correlation is always 1)\n",
    "            ds_corr_no_diag = ds_corr[~np.eye(ds_corr.shape[0], dtype=bool)]\n",
    "            intra_ds_corrs.append(np.mean(ds_corr_no_diag))\n",
    "        \n",
    "        avg_intra_ds_corr = np.mean(intra_ds_corrs)\n",
    "        \n",
    "        # Compute average inter-dataset correlation\n",
    "        inter_ds_corrs = []\n",
    "        for ds1 in range(self.n_datasets):\n",
    "            start_idx1 = ds1 * self.n_features_latent\n",
    "            end_idx1 = start_idx1 + self.n_features_latent\n",
    "            \n",
    "            for ds2 in range(ds1+1, self.n_datasets):\n",
    "                start_idx2 = ds2 * self.n_features_latent\n",
    "                end_idx2 = start_idx2 + self.n_features_latent\n",
    "                \n",
    "                inter_corr = actual_corr[start_idx1:end_idx1, start_idx2:end_idx2]\n",
    "                inter_ds_corrs.append(np.mean(inter_corr))\n",
    "        \n",
    "        avg_inter_ds_corr = np.mean(inter_ds_corrs)\n",
    "        \n",
    "        return avg_intra_ds_corr, avg_inter_ds_corr\n",
    "    \n",
    "    def generate(self):\n",
    "        \"\"\"\n",
    "        Generate datasets with controlled correlation structure.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        list of numpy arrays\n",
    "            Each array has shape [n_samples, n_features_measured]\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"Generating {self.n_datasets} datasets with {self.n_samples} samples each\")\n",
    "            print(f\"Each dataset has {self.n_features_measured} measured features derived from {self.n_features_latent} latent features\")\n",
    "            print(f\"Target intra-dataset correlation: {self.intra_ds_corr}, inter-dataset correlation: {self.inter_ds_corr}\")\n",
    "        \n",
    "        # Step 1: Generate correlated latent factors across all datasets\n",
    "        # We'll create a total of n_datasets * n_features_latent latent factors\n",
    "        total_latent_features = self.n_datasets * self.n_features_latent\n",
    "        \n",
    "        # Create correlation matrix for all latent factors\n",
    "        corr_matrix = np.ones((total_latent_features, total_latent_features))\n",
    "        \n",
    "        # Fill the correlation matrix: intra_ds_corr for features within the same dataset\n",
    "        # and inter_ds_corr for features between different datasets\n",
    "        for i in range(total_latent_features):\n",
    "            for j in range(total_latent_features):\n",
    "                if i == j:\n",
    "                    corr_matrix[i, j] = 1.0  # Diagonal elements are 1\n",
    "                elif i // self.n_features_latent == j // self.n_features_latent:\n",
    "                    # Same dataset\n",
    "                    corr_matrix[i, j] = self.intra_ds_corr\n",
    "                else:\n",
    "                    # Different datasets\n",
    "                    corr_matrix[i, j] = self.inter_ds_corr\n",
    "        \n",
    "        # Ensure the correlation matrix is positive semi-definite (avoid numerical issues)\n",
    "        min_eig = np.min(np.linalg.eigvals(corr_matrix))\n",
    "        if min_eig < 0:\n",
    "            corr_matrix += -min_eig * np.eye(total_latent_features) * 1.1\n",
    "        cov_matrix = corr_matrix\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Generating latent features with the specified correlation structure...\")\n",
    "            \n",
    "        # Generate random samples with the desired correlation structure\n",
    "        self.latent_features = np.random.multivariate_normal(\n",
    "            mean=np.zeros(total_latent_features),\n",
    "            cov=cov_matrix,\n",
    "            size=self.n_samples\n",
    "        )\n",
    "        \n",
    "        # Check the correlations of latent features using our helper method\n",
    "        self.avg_intra_latent_corr, self.avg_inter_latent_corr = self._compute_latent_correlations()\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Average intra-dataset correlation in latent features: {self.avg_intra_latent_corr:.4f} (target: {self.intra_ds_corr:.4f})\")\n",
    "            print(f\"Average inter-dataset correlation in latent features: {self.avg_inter_latent_corr:.4f} (target: {self.inter_ds_corr:.4f})\")\n",
    "        \n",
    "        # Step 2: Generate the measured features for each dataset\n",
    "        if self.verbose:\n",
    "            print(\"\\nProjecting latent features to measured features...\")\n",
    "            \n",
    "        self.datasets = []\n",
    "        \n",
    "        for ds in range(self.n_datasets):\n",
    "            start_idx = ds * self.n_features_latent\n",
    "            end_idx = start_idx + self.n_features_latent\n",
    "            \n",
    "            # Extract latent features for this dataset\n",
    "            ds_latent = self.latent_features[:, start_idx:end_idx]\n",
    "            \n",
    "            if self.n_features_latent < self.n_features_measured:\n",
    "                # Add noise\n",
    "                ds_latent += np.random.normal(0, self.noise_sigma, size=(self.n_samples, self.n_features_latent))\n",
    "    \n",
    "                # We need to project from latent space to measured space\n",
    "                # Generate a random projection matrix\n",
    "                projection_matrix = np.random.normal(0, self.noise_sigma, size=(self.n_features_latent, self.n_features_measured))\n",
    "                \n",
    "                # Normalize the projection matrix columns\n",
    "                projection_matrix /= np.sqrt(np.sum(projection_matrix**2, axis=0))\n",
    "                \n",
    "                # Project the latent features to the measured space\n",
    "                ds_measured = np.dot(ds_latent, projection_matrix)\n",
    "                            \n",
    "                # if self.verbose:\n",
    "                #     print(f\"Dataset {ds+1}: Projected {self.n_features_latent} latent features to {self.n_features_measured} measured features with noise sigma={self.noise_sigma}\")\n",
    "            else:\n",
    "                # When n_features_latent == n_features_measured, we can just take a subset\n",
    "                # (or all) of the latent features as our measured features\n",
    "                ds_measured = ds_latent[:, :self.n_features_measured].copy()\n",
    "                \n",
    "                # Add noise\n",
    "                ds_measured += np.random.normal(0, self.noise_sigma, size=(self.n_samples, self.n_features_measured))\n",
    "                \n",
    "                # if self.verbose:\n",
    "                #     print(f\"Dataset {ds+1}: Used {self.n_features_measured} out of {self.n_features_latent} latent features as measured features with noise sigma={self.noise_sigma}\")\n",
    "            \n",
    "            self.datasets.append(ds_measured)\n",
    "        \n",
    "        # Step 3: Verify the correlation structure of the measured features\n",
    "        if self.verbose:\n",
    "            print(\"\\nVerifying correlation structure of measured features...\")\n",
    "        \n",
    "        # Compute average intra-dataset correlation of measured features\n",
    "        self.avg_intra_measured_corr, self.intra_measured_corrs = self._compute_average_intra_dataset_correlations(self.datasets)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Average intra-dataset correlation in measured features: {self.avg_intra_measured_corr:.4f} (target: {self.intra_ds_corr:.4f})\")\n",
    "        \n",
    "        # Compute average inter-dataset correlation of measured features\n",
    "        self.avg_inter_measured_corr, self.inter_measured_corrs = self._compute_average_inter_dataset_correlations(self.datasets)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Average inter-dataset correlation in measured features: {self.avg_inter_measured_corr:.4f} (target: {self.inter_ds_corr:.4f})\")\n",
    "        \n",
    "        return self.datasets\n",
    "    \n",
    "    def visualize(self, title=None):\n",
    "        \"\"\"\n",
    "        Visualize correlation structure of the datasets.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        title : str, optional\n",
    "            Title for the visualization. If None, a default title will be used.\n",
    "        \"\"\"\n",
    "        if self.datasets is None:\n",
    "            raise ValueError(\"No datasets to visualize. Call generate() first.\")\n",
    "        \n",
    "        if title is None:\n",
    "            if self.n_features_latent < self.n_features_measured:\n",
    "                title = f\"N_FEATURES_LATENT ({self.n_features_latent}) < N_FEATURES_MEASURED ({self.n_features_measured})\"\n",
    "            else:\n",
    "                title = f\"N_FEATURES_LATENT ({self.n_features_latent}) = N_FEATURES_MEASURED ({self.n_features_measured})\"\n",
    "        \n",
    "        sns.set(context=\"notebook\", style='white')\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Plot 1: Intra-dataset correlation heatmaps\n",
    "        ax = axes[0]\n",
    "        \n",
    "        # Create a big correlation matrix of all features within all datasets\n",
    "        all_features = np.hstack(self.datasets)\n",
    "        corr_matrix = np.corrcoef(all_features.T)\n",
    "        \n",
    "        # Add dataset boundaries\n",
    "        boundaries = [0]\n",
    "        for ds in self.datasets:\n",
    "            boundaries.append(boundaries[-1] + ds.shape[1])\n",
    "        \n",
    "        # Create heatmap\n",
    "        sns.heatmap(corr_matrix, ax=ax, cmap=\"PiYG_r\", vmin=-1, vmax=1, \n",
    "                    cbar_kws={\"label\": \"Correlation\"}, square=True)\n",
    "        \n",
    "        # Add lines to separate datasets\n",
    "        for b in boundaries[1:-1]:\n",
    "            ax.axhline(b, color='black', linewidth=1)\n",
    "            ax.axvline(b, color='black', linewidth=1)\n",
    "        \n",
    "        ax.set_title(f\"Corrs. - {title}\")\n",
    "        \n",
    "        # Plot 2: Distribution of correlations\n",
    "        ax = axes[1]\n",
    "        \n",
    "        # Collect all correlation values\n",
    "        intra_corrs = []\n",
    "        inter_corrs = []\n",
    "        \n",
    "        for i, ds1 in enumerate(self.datasets):\n",
    "            # Intra-dataset correlations\n",
    "            corr_matrix = np.corrcoef(ds1.T)\n",
    "            intra_corrs.extend(corr_matrix[np.triu_indices(corr_matrix.shape[0], k=1)])\n",
    "            \n",
    "            # Inter-dataset correlations\n",
    "            for j, ds2 in enumerate(self.datasets[i+1:], i+1):\n",
    "                # Compute correlation between corresponding features\n",
    "                for f1 in range(ds1.shape[1]):\n",
    "                    for f2 in range(ds2.shape[1]):\n",
    "                        corr = np.corrcoef(ds1[:, f1], ds2[:, f2])[0, 1]\n",
    "                        inter_corrs.append(corr)\n",
    "        \n",
    "        # Create kernel density estimates\n",
    "        sns.kdeplot(intra_corrs, ax=ax, label=f\"Intra-ds corrs (mu: {np.mean(intra_corrs):.3f})\", color='magenta', lw=4)\n",
    "        sns.kdeplot(inter_corrs, ax=ax, label=f\"Inter-ds corrs (mu: {np.mean(inter_corrs):.3f})\", color='green', lw=4)\n",
    "        \n",
    "        ax.set_xlabel(\"Correlation Value\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "        ax.set_title(f\"Distribution of Correlations - {title}\")\n",
    "        ax.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return\n",
    "\n",
    "    def get_datasets(self):\n",
    "        \"\"\"\n",
    "        Get the generated datasets.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        list of numpy arrays\n",
    "            Each array has shape [n_samples, n_features_measured]\n",
    "        \"\"\"\n",
    "        if self.datasets is None:\n",
    "            raise ValueError(\"No datasets available. Call generate() first.\")\n",
    "        return self.datasets\n",
    "        \n",
    "    def get_correlation_stats(self):\n",
    "        \"\"\"\n",
    "        Get the correlation statistics for the generated datasets.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary containing various correlation statistics\n",
    "        \"\"\"\n",
    "        if self.datasets is None:\n",
    "            raise ValueError(\"No datasets available. Call generate() first.\")\n",
    "            \n",
    "        return {\n",
    "            \"avg_intra_latent_corr\": self.avg_intra_latent_corr,\n",
    "            \"avg_inter_latent_corr\": self.avg_inter_latent_corr,\n",
    "            \"avg_intra_measured_corr\": self.avg_intra_measured_corr,\n",
    "            \"avg_inter_measured_corr\": self.avg_inter_measured_corr,\n",
    "            \"intra_measured_corrs\": self.intra_measured_corrs,\n",
    "            \"inter_measured_corrs\": self.inter_measured_corrs\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea9653f-14b3-4c45-99c9-5b69cbd4c3e0",
   "metadata": {},
   "source": [
    "# Hyperalignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cfb55e-1963-47de-b9de-03e78dd70e74",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Simulated timeseries data\n",
    "First, we're going to create simulated datasets with a controlled correlational structure. Each dataset has the same number of samples and features and a set between-feature correlation, and added normally distributed noise. Then, across datasets, the same features are correlated to a lesser degree -- mimicing within-participant vs. between-participant correlation structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047fb22f-b3ce-4081-a025-fbd82852dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets with the following parameters\n",
    "n_participants, n_samples, n_features = 12, 100, 3\n",
    "within_corr = 0.8\n",
    "between_corr = 0.4\n",
    "noise=0.1\n",
    "\n",
    "simulator = SimulatedCorrelatedData(n_datasets=n_participants, \n",
    "                        n_samples=n_samples, \n",
    "                        n_features_measured=n_features,\n",
    "                        n_features_latent=2,\n",
    "                       noise_sigma=noise,\n",
    "                       intra_ds_corr=within_corr,\n",
    "                       inter_ds_corr=between_corr,\n",
    "                       verbose=1)\n",
    "dss = simulator.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865d9037-f775-4ed4-867f-1922bfa9fc11",
   "metadata": {},
   "source": [
    "Let's visually verify the structure of these datasets. In the matrix, features within-dataset are along the diagonal -- so the first 20 features on the x,y axes belong to participant 1, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c312bb-8ec5-4901-ad79-83a4c792a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e5ec83-d556-4e5e-8242-f669c8d379e4",
   "metadata": {},
   "source": [
    "Because these data are 3D, we can plot them for each participant, where each axis is feature, each participant is a color, and each point is a sample. We've included code to plot these as a) scatterplots and b) trajectories in the `demo_data_helpers` file, and will plot them here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f422fc-8a95-4f02-9d1a-eb81400c4507",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_3d_datasets(\n",
    "        datasets=dss,\n",
    "        view_angles=(20, 30),\n",
    "    s=2,\n",
    "    title='datasets pre alignment'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a29f0c-1f0e-4c99-8820-fccc14988bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_3d_trajectories(\n",
    "        trajectories=dss,\n",
    "        view_angles=(20, 30),\n",
    "    title='Trajectories pre alignment'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67089d14-0591-44aa-befa-93f52f511a70",
   "metadata": {},
   "source": [
    "As you can see, these datasets seem pretty misaligned along the axes of these three features. Now, let's try hyperaligning this data. Hyperalignment will find a high-dimensional template that captures similarities between the datasets, and derive transformations between the individual datasets to that high-dimensional template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7e9bfc-69ec-4824-a469-2f57ed56d889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, let's use all the default settings\n",
    "hyp_model = Hyperalignment(verbose=True)\n",
    "hyp_model.fit(dss)\n",
    "isc_results=hyp_model.evaluate_isc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3827e-0d84-471b-baeb-0e510c6f6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_3d_datasets(\n",
    "        datasets=hyp_model.aligned_data,\n",
    "    feature_names=['dim1','dim2','dim3'],\n",
    "    title='datasets after alignment',\n",
    "        view_angles=(20, 30)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edf8397-daa1-4642-a3ca-4e5d1dc2028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_3d_trajectories(\n",
    "        trajectories=hyp_model.aligned_data,\n",
    "        view_angles=(20, 30),\n",
    "    feature_names=['dim1','dim2','dim3'],\n",
    "        title='trajectory post alignment',\n",
    "        linewidth=1             # Width of the trajectory lines\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a325b3-36a7-46a1-8174-30e97768ece8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Movie timeseries data\n",
    "Next we're going to look at data collected while 17 participants watched an episode of BBC's Sherlock in the scanner (accessed from : http://arks.princeton.edu/ark:/88435/dsp01nz8062179). These data were already preprocessed and extracted from an early visual ROI. We are going to demonstrate how to run hyperalignment with cross-validation on these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf8e004-766d-4f34-b0eb-81cc652e214b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dss = load_sherlock_movie()\n",
    "labels = load_sherlock_labels()\n",
    "n_participants, n_trs, n_voxels = dss.shape # 16 participants ,1976 timepoints, 307 voxels\n",
    "print(dss.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855502a9-11c0-4fe3-a013-31e3e53f3621",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Split-half cross validation\n",
    "For the first form of cross-validation, we are going to use one half of the timeseries, for all participants, to train the hyperalignment model and transformation matrices. Then, we are going to apply those transformation matrices to the second half of the data, and evaluate ISC and between-subject classification accuracy. Cross-validation like this would be used to evaluate that the transformations learned with hyperalignment aren't overfit to the data it received during training, but actually represent something generalizable about each participants' functional organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d3cca-1005-442e-b046-b2c275877595",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dss = dss[:, :n_trs//2, :] # training on first half\n",
    "testing_dss = dss[:, n_trs//2:, :] # testing on second half\n",
    "hyp_model = Hyperalignment(verbose=0)\n",
    "hyp_model.fit(training_dss)\n",
    "\n",
    "# Apply those transformations to the second half\n",
    "testing_dss_aligned = hyp_model.transform(testing_dss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba565375-5c3e-4c7b-85c9-4d832eab340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_results = hyp_model._run_isc(testing_dss)\n",
    "testing_results_aligned = hyp_model._run_isc(testing_dss_aligned)\n",
    "training_results_dict=hyp_model.evaluate_isc()\n",
    "training_results = training_results_dict['Pre-alignment']\n",
    "training_results_aligned = training_results_dict['Post-alignment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b71cf-62e3-4503-be32-778f87427b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average results to get one per subject per split\n",
    "r0 = np.mean(testing_results,axis=1)\n",
    "r1 = np.mean(testing_results_aligned, axis=1)\n",
    "r2 = np.mean(training_results, axis=1)\n",
    "r3 = np.mean(training_results_aligned, axis=1)\n",
    "isc_results = {'scores': np.concatenate([r0,r1,r2,r3]), \n",
    "               'participants':np.tile(np.arange(1,17), 4),\n",
    "              'aligned':np.concatenate([np.zeros(16), np.ones(16), np.zeros(16), np.ones(16)]), \n",
    "              'training':np.concatenate([np.zeros(16), np.zeros(16), np.ones(16), np.ones(16)])}\n",
    "isc_results['aligned_labels'] = ['pre-' if v == 0 else 'post-' for v in isc_results['aligned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c3b6af-be2d-4d70-b2e9-2d95feb2970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(context='notebook', style='white')\n",
    "g=sns.barplot(data=isc_results, x='training', \n",
    "              y='scores', hue='aligned_labels',\n",
    "              palette='mako',alpha=0.8, \n",
    "              edgecolor='k', linewidth=2)\n",
    "g=sns.stripplot(data=isc_results, x='training', y='scores', \n",
    "                dodge=True, hue='aligned_labels',\n",
    "                legend=False, palette='mako', edgecolor='k', \n",
    "                linewidth=1, )\n",
    "g.set(xticks=[0,1],xticklabels=['testing','training'], ylim=[0,0.8], xlabel='', \n",
    "      ylabel='Correlation', title='ISC in training and testing sets, pre/post HA')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782003ea-5dc1-44f4-994d-5b5709bfbbd3",
   "metadata": {},
   "source": [
    "What we see here is that hyperalignment improves how synchronized participants' brain responses are with one another. That improvement is greatest in the data that were used to train the hyperalignment (`training` on the x-axis), but the transformations also generalized -- when applied to unseen data (`testing` on the x-axis), we also saw a boost in synchrony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c809d371-0aa2-43cc-8767-5dc1e9f5778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_upsample_train_data(X_train, Y_train):\n",
    "    '''\n",
    "    Randomly upsamples training samples so that there are equal samples for each class \n",
    "    Helpful for balancing classification problems\n",
    "    '''\n",
    "    (unique, counts) = np.unique(Y_train, return_counts=True)\n",
    "    goal = max(counts)\n",
    "    train_indices = []\n",
    "    for ind, classs in enumerate(unique):\n",
    "        if counts[ind] != goal:\n",
    "            num2add = goal - counts[ind]\n",
    "            where_this_class = np.where(Y_train == classs)\n",
    "            chosen_indices = list(random.choices(where_this_class[0], k=num2add)) + list(where_this_class[0])\n",
    "            train_indices = train_indices + chosen_indices\n",
    "        else:\n",
    "            train_indices += list(np.where(Y_train == classs)[0])\n",
    "    train_indices.sort()\n",
    "    X_train = X_train[train_indices]\n",
    "    Y_train = Y_train[train_indices]\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e7321-ae21-4e10-95ad-8972f64b0f83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=['fold','aligned','accuracy'])\n",
    "label_subset = labels[n_trs//2:]\n",
    "training_labels_orig = np.tile(label_subset, 12)\n",
    "label_subset1=np.tile(label_subset, 4)\n",
    "folder=KFold(n_splits=4)\n",
    "testing_dss_aligned=np.array(testing_dss_aligned)\n",
    "testing_dss = np.array(testing_dss)\n",
    "_, ns, nv = testing_dss.shape\n",
    "kernel='rbf'\n",
    "C=10\n",
    "# between subject classification analysis \n",
    "for i,(train_idx,test_idx) in enumerate(folder.split(np.arange(16))):\n",
    "    print(f'Fold {i}, testing on: {test_idx}')\n",
    "\n",
    "    # train on all other subjects' data; test on held-out    \n",
    "    clf_training, clf_testing = testing_dss[train_idx].reshape(len(train_idx)*ns,nv), testing_dss[test_idx].reshape(len(test_idx)*ns, nv)\n",
    "    #clf_training, training_labels = random_upsample_train_data(clf_training, training_labels_orig)\n",
    "    training_labels=training_labels_orig\n",
    "    clf_training_aligned, clf_testing_aligned = testing_dss_aligned[train_idx].reshape(len(train_idx)*ns,nv), testing_dss_aligned[test_idx].reshape(len(test_idx)*ns, nv)\n",
    "    #clf_training_aligned, training_labels = random_upsample_train_data(clf_training_aligned, training_labels_orig)\n",
    "\n",
    "    print(f'reshaped data: {clf_training.shape}, {clf_testing.shape}, {clf_training_aligned.shape}, {clf_testing_aligned.shape}')\n",
    "    svc_orig = svm.SVC(C=C, kernel=kernel,class_weight='balanced')\n",
    "    svc_orig.fit(clf_training, training_labels)\n",
    "    acc = svc_orig.score(clf_testing, label_subset1)\n",
    "    print(f'orig={np.round(acc,3)}')\n",
    "    df.loc[len(df)] = {'fold':i, 'aligned':'pre-', 'accuracy':acc}\n",
    "\n",
    "    svc_align = svm.SVC(C=C, kernel=kernel, class_weight='balanced')\n",
    "    svc_align.fit(clf_training_aligned, training_labels)\n",
    "    acc_align = svc_align.score(clf_testing_aligned, label_subset1)\n",
    "    df.loc[len(df)] = {'fold':i, 'aligned':'post-', 'accuracy':acc_align}\n",
    "    print(f'aligned={np.round(acc_align,3)}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e6691-b266-4cc3-b8af-ec18ab34daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1,figsize=(2,4))\n",
    "g=sns.barplot(data=df, x='aligned', \n",
    "              y='accuracy', \n",
    "              order=['pre-','post-'],\n",
    "              palette='mako',alpha=0.8, \n",
    "              edgecolor='k', linewidth=2)\n",
    "\n",
    "g.set(xticklabels=['pre-','post-'], ylim=[0,1], xlabel='', \n",
    "      ylabel='accuracy', title='Between-subject classification')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c611d6-fb42-46da-87c4-1efbb9a02079",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Aligning RS-fMRI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba700dc-4706-46fb-8fb9-2488ce3baa00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded_data=load_rsfmri(20, seed_region=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c3b34-8b33-42ee-924d-e348d67c06db",
   "metadata": {},
   "outputs": [],
   "source": [
    "parcellated_data, region_data = np.array(loaded_data['parcellated_all']), np.array(loaded_data['region_all'])\n",
    "n_timepoints = region_data.shape[1]\n",
    "parcellated_split_0, region_split_0 = parcellated_data[:, :n_timepoints//2, :], region_data[:, :n_timepoints//2, :]\n",
    "parcellated_split_1, region_split_1 = parcellated_data[:, n_timepoints//2:, :], region_data[:, n_timepoints//2:, :]\n",
    "\n",
    "cnx_split0 = [np.nan_to_num(compute_connectivity_matrix(p.T, r.T)) for p,r in zip(parcellated_split_0, region_split_0)]\n",
    "cnx_split1 = [np.nan_to_num(compute_connectivity_matrix(p.T, r.T)) for p,r in zip(parcellated_split_1, region_split_1)]\n",
    "print(np.shape(cnx_split0), np.shape(cnx_split1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef46a03-cf2b-4f5a-8379-0e8bdb43741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train hyperalignment on the first half and transform the second half, and vice-versa\n",
    "model_split0 = hyp.Hyperalignment(verbose=0)\n",
    "model_split0.fit(cnx_split0)\n",
    "aligned_cnx_split1 = model_split0.apply_fit_transformations(cnx_split1)\n",
    "\n",
    "model_split1 = hyp.Hyperalignment(verbose=0)\n",
    "model_split1.fit(cnx_split1)\n",
    "aligned_cnx_split0 = model_split1.apply_fit_transformations(cnx_split0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73a2bc-a22d-4d88-994f-ab0a37e9c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate reliability of connectivity across split-halves\n",
    "def individual_differences_reliability(cnx0, cnx1):\n",
    "    pcorr0 = compute_pairwise_corr(cnx0)\n",
    "    pcorr1 = compute_pairwise_corr(cnx1)\n",
    "    return np.corrcoef(pcorr0, pcorr1)[0,1]\n",
    "   \n",
    "def compute_pairwise_corr(c):\n",
    "    n,s,t=np.array(c).shape\n",
    "    c = np.array(c).reshape(n,s*t)\n",
    "    corrs = pdist(c, 'correlation')\n",
    "    return corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c3dcc-e5e6-414c-b988-456fdac22c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running reliability of individual differences across the split halves of data, \n",
    "# before and after hyperalignment\n",
    "\n",
    "pcorr0 = squareform(compute_pairwise_corr(cnx_split0))\n",
    "pcorr1 = squareform(compute_pairwise_corr(cnx_split1))\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,4))\n",
    "r=individual_differences_reliability(cnx_split0, cnx_split1)\n",
    "g=sns.heatmap(pcorr0, ax=ax[0], vmin=0, vmax=1, cmap='viridis', square=True)\n",
    "g.set(title='Pre-HA conn ISC split 0')\n",
    "g=sns.heatmap(pcorr1, ax=ax[1], vmin=0, vmax=1, cmap='viridis', square=True)\n",
    "g.set(title=f'split 1; split-half rel ={np.round(r,3)}')\n",
    "\n",
    "\n",
    "apcorr0 = squareform(compute_pairwise_corr(aligned_cnx_split0))\n",
    "apcorr1 = squareform(compute_pairwise_corr(aligned_cnx_split1))\n",
    "r=individual_differences_reliability(aligned_cnx_split0, aligned_cnx_split1)\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,4))\n",
    "g=sns.heatmap(apcorr0, ax=ax[0], vmin=0, vmax=1, cmap='viridis', square=True)\n",
    "g.set(title='Post-HA conn ISC split 0')\n",
    "g=sns.heatmap(apcorr1, ax=ax[1], vmin=0, vmax=1, cmap='viridis', square=True)\n",
    "g.set(title=f'split 1; split-half rel ={np.round(r,3)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd0aab9-d498-4d1b-a9ff-a83802a5a6ba",
   "metadata": {},
   "source": [
    "So what we see here is that the relationships between participants' connectivity information (i.e., how different participants' functional connectomes are from one another) are much more reliable after hyperalignment than before hyperalignment (pre HA: $r=0.288$ versus post HA: $r=0.591$). These data were hyperaligned across split-halves (i.e. training the hyperalignment model on connectivity matrices computed from the first half of the resting state timeseries and using it to transform the second half, and vice-versa, as in our recent [J. Neurosci paper](https://ericabusch.github.io/files/abcd_ha.pdf)). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe5e306-0d1b-4663-82a3-5a20c0dc6187",
   "metadata": {},
   "source": [
    "# Shared response model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6408f7-1f97-44b5-bf20-780533730558",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Simulated timeseries data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95740b11-5684-4df5-bcff-bfd9ec0fad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_isc(X):\n",
    "    '''\n",
    "    Intersubject correlation analysis: \n",
    "    compares each subject's timeseries with the average of all other subjects' timeseries\n",
    "    Measure of \"synchrony\" over time across brains, which should be improved by functional alignment\n",
    "    '''\n",
    "    if len(np.shape(X)) == 2:\n",
    "        X = X[:,:,np.newaxis]\n",
    "    results = np.zeros((len(X), X[0].shape[-1]))\n",
    "    for i in range(len(X)):\n",
    "        test = X[i]\n",
    "        tr_idx = np.setdiff1d(np.arange(len(X)),i)\n",
    "        train = np.mean(np.array([X[j] for j in tr_idx]), axis=0)\n",
    "        # get correlation at each feature\n",
    "        cmat = cdist(test.T, train.T, 'correlation')\n",
    "        corrs = [1-cmat[i,i] for i in range(cmat.shape[0])]\n",
    "        results[i]= corrs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd1e0e-7d3d-45e1-80cd-cb12364b0b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets with the following parameters\n",
    "n_participants, n_samples, n_features = 12, 100, 20\n",
    "within_corr = 0.8\n",
    "between_corr = 0.2\n",
    "noise=0.1\n",
    "\n",
    "simulator = SimulatedCorrelatedData(n_datasets=n_participants, \n",
    "                        n_samples=n_samples, \n",
    "                        n_features_measured=n_features,\n",
    "                        n_features_latent=2,\n",
    "                       noise_sigma=noise,\n",
    "                       intra_ds_corr=within_corr,\n",
    "                       inter_ds_corr=between_corr,\n",
    "                       verbose=0)\n",
    "dss = simulator.generate()\n",
    "\n",
    "# Fit model with 5 features\n",
    "srm_model = srm.SRM(features=5)\n",
    "\n",
    "# Have to reshape to be [subjects, features, samples] instead\n",
    "srm_model.fit([d.T for d in dss])\n",
    "\n",
    "# Transform the data\n",
    "srm_data = srm_model.transform([d.T for d in dss])\n",
    "srm_data = [d.T for d in srm_data]\n",
    "iscs_srmed = np.mean(run_isc(srm_data),axis=1)\n",
    "iscs_pre = np.mean(run_isc(dss),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647eed29-58df-4182-a1be-3c1610a8c4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a270529-b96d-4f0b-a2f6-d24368efdb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1,figsize=(2,4))\n",
    "\n",
    "g=sns.barplot([iscs_pre, iscs_srmed], palette='mako', ec='k')\n",
    "g.set(xticklabels=['pre-srm','post-srm'], ylabel='correlation')\n",
    "g=sns.stripplot([iscs_pre, iscs_srmed], palette='mako', dodge=False, edgecolor='k', \n",
    "                linewidth=1,)\n",
    "g.set(xticklabels=['pre-srm','post-srm'], ylabel='correlation', title='ISC, simulated data')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26420658-7add-4a30-92b3-de326d6363fa",
   "metadata": {},
   "source": [
    "## Movie timeseries data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce4d894-34cc-467e-9a92-0f3595347386",
   "metadata": {},
   "outputs": [],
   "source": [
    "dss = load_sherlock_movie()\n",
    "labels = load_sherlock_labels()\n",
    "n_participants, n_trs, n_voxels = dss.shape # 16 participants ,1976 timepoints, 307 voxels\n",
    "print(dss.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d259f-5b6c-4c9e-94eb-1516eeba32ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dss = stats.zscore(dss[:, :n_trs//2, :],axis=1) # training on first half,zscore across timepoints\n",
    "testing_dss = stats.zscore(dss[:, n_trs//2:, :],axis=1) # testing on second half\n",
    "srm_model = srm.SRM(features=50)\n",
    "srm_model.fit([d.T for d in training_dss])\n",
    "\n",
    "# Transform the training data\n",
    "srm_training_dss = srm_model.transform([d.T for d in training_dss])\n",
    "srm_training_dss = np.array([d.T for d in srm_training_dss])\n",
    "\n",
    "# Transform the testing data\n",
    "srm_testing_dss = srm_model.transform([d.T for d in testing_dss])\n",
    "srm_testing_dss = np.array([d.T for d in srm_testing_dss])\n",
    "\n",
    "# Evaluate intersubject correlations\n",
    "training_isc = run_isc(training_dss)\n",
    "testing_isc = run_isc(testing_dss)\n",
    "srm_training_isc = run_isc(srm_training_dss)\n",
    "srm_testing_isc = run_isc(srm_testing_dss)\n",
    "\n",
    "# Average results to get one per subject per split\n",
    "r0 = np.mean(training_isc,axis=1)\n",
    "r1 = np.mean(testing_isc, axis=1)\n",
    "r2 = np.mean(srm_training_isc, axis=1)\n",
    "r3 = np.mean(srm_testing_isc, axis=1)\n",
    "isc_results = {'scores': np.concatenate([r0,r1,r2,r3]), \n",
    "               'participants':np.tile(np.arange(1,17), 4),\n",
    "              'aligned':np.concatenate([np.zeros(16), np.zeros(16), np.ones(16), np.ones(16)]), \n",
    "              'training':np.concatenate([np.ones(16), np.zeros(16), np.ones(16), np.zeros(16)])}\n",
    "isc_results['aligned_labels'] = ['pre-' if v == 0 else 'post-' for v in isc_results['aligned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97053737-9a37-4394-853c-f491b9aabd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(context='notebook', style='white')\n",
    "g=sns.barplot(data=pd.DataFrame(isc_results), x='training', \n",
    "              y='scores', hue='aligned_labels',\n",
    "              palette='mako',alpha=0.8, \n",
    "              edgecolor='k', linewidth=2)\n",
    "g=sns.stripplot(data=isc_results, x='training', y='scores', \n",
    "                dodge=True, hue='aligned_labels',\n",
    "                legend=False, palette='mako', edgecolor='k', \n",
    "                linewidth=1, )\n",
    "g.set(xticks=[0,1],xticklabels=['testing','training'], ylim=[0,0.8], xlabel='', \n",
    "      ylabel='Correlation', title='ISC in training and testing sets, pre/post SRM')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250ee44-6937-444e-acc9-ed64cc56bc60",
   "metadata": {},
   "source": [
    "Something else we can do with SRM is tune the number of shared responses learned, with cross-validation. This is how it would work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f6026-2834-4679-ac04-d6d28a0f5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through feature hyperparameter and compute ISC on the training data\n",
    "K_values = np.arange(5, 100, 5)\n",
    "average_iscs = np.zeros(len(K_values))\n",
    "for i,k in enumerate(K_values):\n",
    "    srm_model = srm.SRM(features=k)\n",
    "    srm_model.fit([d.T for d in training_dss])\n",
    "    # now score ISC on the training data\n",
    "    dss_srm = srm_model.transform([d.T for d in training_dss])\n",
    "    corr = np.mean(run_isc(dss_srm))\n",
    "    average_iscs[i] = corr\n",
    "\n",
    "# Choose the K that resulted in the highest isc\n",
    "maxx = np.argmax(average_iscs)\n",
    "best_k = K_values[maxx]\n",
    "srm_model = srm.SRM(features=k)\n",
    "srm_model.fit([d.T for d in training_dss])\n",
    "srm_training_dss = srm_model.transform([d.T for d in training_dss])\n",
    "srm_testing_dss = srm_model.transform([d.T for d in testing_dss])\n",
    "\n",
    "# Evaluate intersubject correlations\n",
    "training_isc = run_isc(training_dss)\n",
    "testing_isc = run_isc(testing_dss)\n",
    "srm_training_isc = run_isc(srm_training_dss)\n",
    "srm_testing_isc = run_isc(srm_testing_dss)\n",
    "\n",
    "# Average results to get one per subject per split\n",
    "r0 = np.mean(training_isc,axis=1)\n",
    "r1 = np.mean(testing_isc, axis=1)\n",
    "r2 = np.mean(srm_training_isc, axis=1)\n",
    "r3 = np.mean(srm_testing_isc, axis=1)\n",
    "isc_results = {'scores': np.concatenate([r0,r1,r2,r3]), \n",
    "               'participants':np.tile(np.arange(1,17), 4),\n",
    "              'aligned':np.concatenate([np.zeros(16), np.zeros(16), np.ones(16), np.ones(16)]), \n",
    "              'training':np.concatenate([np.ones(16), np.zeros(16), np.ones(16), np.zeros(16)])}\n",
    "isc_results['aligned_labels'] = ['pre-' if v == 0 else 'post-' for v in isc_results['aligned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad65ed37-3d0d-4dec-8f78-385d3e086d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(context='notebook', style='white')\n",
    "g=sns.barplot(data=isc_results, x='training', \n",
    "              y='scores', hue='aligned_labels',\n",
    "              palette='mako',alpha=0.8, \n",
    "              edgecolor='k', linewidth=2)\n",
    "g=sns.stripplot(data=isc_results, x='training', y='scores', \n",
    "                dodge=True, hue='aligned_labels',\n",
    "                legend=False, palette='mako', edgecolor='k', \n",
    "                linewidth=1, )\n",
    "g.set(xticks=[0,1],xticklabels=['testing','training'], ylim=[0,0.8], xlabel='', \n",
    "      ylabel='Correlation', title=f'ISC in training and testing sets, pre/post SRM with K={best_k} tuned')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ee945-1323-4c7c-8cc1-4d3d1507689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.DataFrame(columns=['fold','aligned','accuracy'])\n",
    "label_subset = labels[n_trs//2:]\n",
    "training_labels_orig = np.tile(label_subset, 15)\n",
    "label_subset1=label_subset#np.tile(label_subset, 4)\n",
    "folder=LeaveOneOut()#KFold(n_splits=4)\n",
    "testing_dss_aligned=np.array(testing_dss_aligned)\n",
    "testing_dss = np.array(testing_dss)\n",
    "_, ns, nv = testing_dss.shape\n",
    "kernel='rbf'\n",
    "C=10\n",
    "# between subject classification analysis \n",
    "for i,(train_idx,test_idx) in enumerate(folder.split(np.arange(16))):\n",
    "    print(f'Fold {i}, testing on: {test_idx}')\n",
    "    \n",
    "    clf_training_aligned, clf_testing_aligned = testing_dss_aligned[train_idx].reshape(len(train_idx)*ns,nv), testing_dss_aligned[test_idx].reshape(len(test_idx)*ns, nv)\n",
    "    clf_training_aligned, training_labels = random_upsample_train_data(clf_training_aligned, training_labels_orig)\n",
    "\n",
    "    svc_align = svm.SVC(C=C, kernel=kernel, class_weight='balanced')\n",
    "    svc_align.fit(clf_training_aligned, training_labels)\n",
    "    acc_align = svc_align.score(clf_testing_aligned, label_subset1)\n",
    "    df1.loc[len(df1)] = {'fold':i, 'aligned':'post-', 'accuracy':acc_align}\n",
    "    print(f'aligned={np.round(acc_align,3)}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98415ff-1b2e-4177-990b-40ce2dfe2fb1",
   "metadata": {},
   "source": [
    "## Aligning RS-fMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e4015-8103-4edb-bc3b-38e2e9642209",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data=load_rsfmri(20, seed_region=10)\n",
    "parcellated_data, region_data = np.array(loaded_data['parcellated_all']), np.array(loaded_data['region_all'])\n",
    "n_timepoints = region_data.shape[1]\n",
    "parcellated_split_0, region_split_0 = parcellated_data[:, :n_timepoints//2, :], region_data[:, :n_timepoints//2, :]\n",
    "parcellated_split_1, region_split_1 = parcellated_data[:, n_timepoints//2:, :], region_data[:, n_timepoints//2:, :]\n",
    "\n",
    "cnx_split0 = [np.nan_to_num(compute_connectivity_matrix(p.T, r.T)) for p,r in zip(parcellated_split_0, region_split_0)]\n",
    "cnx_split1 = [np.nan_to_num(compute_connectivity_matrix(p.T, r.T)) for p,r in zip(parcellated_split_1, region_split_1)]\n",
    "print(np.shape(cnx_split0), np.shape(cnx_split1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b18c1f-ad8e-4a96-a850-e894365295ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SRM on the first half and transform the second half, and vice-versa\n",
    "\n",
    "cnx_split1T = np.array([c for c in cnx_split1])\n",
    "cnx_split0T = np.array([c for c in cnx_split0]) # reshape \n",
    "\n",
    "model_split0 = srm.SRM(features=20)\n",
    "model_split0.fit(cnx_split0T)\n",
    "\n",
    "aligned_cnx_split1 = model_split0.transform(cnx_split1T)\n",
    "aligned_cnx_split1 = np.array([s for s in aligned_cnx_split1])\n",
    "\n",
    "\n",
    "model_split1 = srm.SRM(features=20)\n",
    "model_split1.fit(cnx_split1T)\n",
    "\n",
    "aligned_cnx_split0 = model_split1.transform(cnx_split0T)\n",
    "aligned_cnx_split0 = np.array([s for s in aligned_cnx_split0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e7879a-d32b-45ab-ae09-456637f8ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate reliability of connectivity across split-halves\n",
    "def individual_differences_reliability(cnx0, cnx1):\n",
    "    pcorr0 = compute_pairwise_corr(cnx0)\n",
    "    pcorr1 = compute_pairwise_corr(cnx1)\n",
    "    return np.corrcoef(pcorr0, pcorr1)[0,1]\n",
    "   \n",
    "def compute_pairwise_corr(c):\n",
    "    n,s,t=np.array(c).shape\n",
    "    c = np.array(c).reshape(n,s*t)\n",
    "    corrs = pdist(c, 'correlation')\n",
    "    return corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ad9e1-c2ad-4094-bb97-ccb1d65c73f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running reliability of individual differences across the split halves of data, \n",
    "# before and after hyperalignment\n",
    "\n",
    "pcorr0 = squareform(compute_pairwise_corr(cnx_split0))\n",
    "pcorr1 = squareform(compute_pairwise_corr(cnx_split1))\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,4))\n",
    "r=individual_differences_reliability(cnx_split0, cnx_split1)\n",
    "g=sns.heatmap(pcorr0, ax=ax[0], vmin=0, vmax=1.5, cmap='viridis', square=True)\n",
    "g.set(title='Pre-SRM corr. diff. split 0')\n",
    "g=sns.heatmap(pcorr1, ax=ax[1], vmin=0, vmax=1.5, cmap='viridis', square=True)\n",
    "g.set(title=f'Pre-SRM corr. diff. split 1; rel = {np.round(r,3)}')\n",
    "\n",
    "\n",
    "apcorr0 = squareform(compute_pairwise_corr(aligned_cnx_split0))\n",
    "apcorr1 = squareform(compute_pairwise_corr(aligned_cnx_split1))\n",
    "r=individual_differences_reliability(aligned_cnx_split0, aligned_cnx_split1)\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,4))\n",
    "g=sns.heatmap(apcorr0, ax=ax[0], vmin=0, vmax=1.5, cmap='viridis', square=True)\n",
    "g.set(title=f'Post-SRM corr. diff. split 0')\n",
    "g=sns.heatmap(apcorr1, ax=ax[1], vmin=0, vmax=1.5, cmap='viridis', square=True)\n",
    "g.set(title=f'Post-SRM corr. diff. split 1; rel = {np.round(r,3)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b86a3-3f17-42d2-aed4-6f6be8ba3296",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
