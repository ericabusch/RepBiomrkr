{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e045ff-1312-4009-9a64-c36f15300aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import svm\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from sklearn.model_selection import KFold, LeaveOneOut\n",
    "import random\n",
    "import hyperalignment as hyp\n",
    "from brainiak.funcalign import srm\n",
    "from simulate_correlated_data import SimulatedCorrelatedData\n",
    "from demo_data_helpers import (load_rsfmri, load_sherlock_movie, \n",
    "                                compute_connectivity_matrix, plot_3d_datasets, \n",
    "                               plot_3d_trajectories, load_sherlock_labels)\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.colors as mcolors\n",
    "random.seed(4)\n",
    "np.random.seed(4)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea9653f-14b3-4c45-99c9-5b69cbd4c3e0",
   "metadata": {},
   "source": [
    "# Hyperalignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cfb55e-1963-47de-b9de-03e78dd70e74",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Simulated timeseries data\n",
    "First, we're going to create simulated datasets with a controlled correlational structure. Each dataset has the same number of samples and features and a set between-feature correlation, and added normally distributed noise. Then, across datasets, the same features are correlated to a lesser degree -- mimicing within-participant vs. between-participant correlation structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047fb22f-b3ce-4081-a025-fbd82852dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets with the following parameters\n",
    "n_participants, n_samples, n_features = 12, 100, 3\n",
    "within_corr = 0.8\n",
    "between_corr = 0.4\n",
    "noise=0.1\n",
    "\n",
    "simulator = SimulatedCorrelatedData(n_datasets=n_participants, \n",
    "                        n_samples=n_samples, \n",
    "                        n_features_measured=n_features,\n",
    "                        n_features_latent=2,\n",
    "                       noise_sigma=noise,\n",
    "                       intra_ds_corr=within_corr,\n",
    "                       inter_ds_corr=between_corr,\n",
    "                       verbose=1)\n",
    "dss = simulator.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865d9037-f775-4ed4-867f-1922bfa9fc11",
   "metadata": {},
   "source": [
    "Let's visually verify the structure of these datasets. In the matrix, features within-dataset are along the diagonal -- so the first 20 features on the x,y axes belong to participant 1, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c312bb-8ec5-4901-ad79-83a4c792a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e5ec83-d556-4e5e-8242-f669c8d379e4",
   "metadata": {},
   "source": [
    "Because these data are 3D, we can plot them for each participant, where each axis is feature, each participant is a color, and each point is a sample. We've included code to plot these as a) scatterplots and b) trajectories in the `demo_data_helpers` file, and will plot them here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f422fc-8a95-4f02-9d1a-eb81400c4507",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_3d_datasets(\n",
    "        datasets=dss,\n",
    "        view_angles=(20, 30),\n",
    "    s=2,\n",
    "    title='datasets pre alignment'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a29f0c-1f0e-4c99-8820-fccc14988bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_3d_trajectories(\n",
    "        trajectories=dss,\n",
    "        view_angles=(20, 30),\n",
    "    title='Trajectories pre alignment'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67089d14-0591-44aa-befa-93f52f511a70",
   "metadata": {},
   "source": [
    "As you can see, these datasets seem pretty misaligned along the axes of these three features. Now, let's try hyperaligning this data. Hyperalignment will find a high-dimensional template that captures similarities between the datasets, and derive transformations between the individual datasets to that high-dimensional template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7e9bfc-69ec-4824-a469-2f57ed56d889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, let's use all the default settings\n",
    "hyp_model = hyp.Hyperalignment(verbose=True)\n",
    "hyp_model.fit(dss)\n",
    "isc_results=hyp_model.evaluate_isc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3827e-0d84-471b-baeb-0e510c6f6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_3d_datasets(\n",
    "        datasets=hyp_model.aligned_data,\n",
    "    feature_names=['dim1','dim2','dim3'],\n",
    "    title='datasets after alignment',alpha=0.5,\n",
    "        view_angles=(20, 30)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edf8397-daa1-4642-a3ca-4e5d1dc2028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_3d_trajectories(\n",
    "        trajectories=hyp_model.aligned_data,\n",
    "        view_angles=(20, 30),\n",
    "    feature_names=['dim1','dim2','dim3'],\n",
    "        title='trajectory post alignment',alpha=0.5,\n",
    "        linewidth=1             # Width of the trajectory lines\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a325b3-36a7-46a1-8174-30e97768ece8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Movie timeseries data\n",
    "Next we're going to look at data collected while 17 participants watched an episode of BBC's Sherlock in the scanner (accessed from : http://arks.princeton.edu/ark:/88435/dsp01nz8062179). These data were already preprocessed and extracted from an early visual ROI. We are going to demonstrate how to run hyperalignment with cross-validation on these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf8e004-766d-4f34-b0eb-81cc652e214b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dss = load_sherlock_movie()\n",
    "labels = load_sherlock_labels()\n",
    "n_participants, n_trs, n_voxels = dss.shape # 16 participants ,1976 timepoints, 307 voxels\n",
    "print(dss.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855502a9-11c0-4fe3-a013-31e3e53f3621",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Split-half cross validation\n",
    "For the first form of cross-validation, we are going to use one half of the timeseries, for all participants, to train the hyperalignment model and transformation matrices. Then, we are going to apply those transformation matrices to the second half of the data, and evaluate ISC and between-subject classification accuracy. Cross-validation like this would be used to evaluate that the transformations learned with hyperalignment aren't overfit to the data it received during training, but actually represent something generalizable about each participants' functional organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d3cca-1005-442e-b046-b2c275877595",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dss = dss[:, :n_trs//2, :] # training on first half\n",
    "testing_dss = dss[:, n_trs//2:, :] # testing on second half\n",
    "hyp_model = hyp.Hyperalignment(verbose=0)\n",
    "hyp_model.fit(training_dss)\n",
    "\n",
    "# Apply those transformations to the second half\n",
    "testing_dss_aligned = hyp_model.transform(testing_dss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba565375-5c3e-4c7b-85c9-4d832eab340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_results = hyp_model._run_isc(testing_dss)\n",
    "testing_results_aligned = hyp_model._run_isc(testing_dss_aligned)\n",
    "training_results_dict=hyp_model.evaluate_isc()\n",
    "training_results = training_results_dict['Pre-alignment']\n",
    "training_results_aligned = training_results_dict['Post-alignment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b71cf-62e3-4503-be32-778f87427b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average results to get one per subject per split\n",
    "r0 = np.mean(testing_results,axis=1)\n",
    "r1 = np.mean(testing_results_aligned, axis=1)\n",
    "r2 = np.mean(training_results, axis=1)\n",
    "r3 = np.mean(training_results_aligned, axis=1)\n",
    "isc_results = {'scores': np.concatenate([r0,r1,r2,r3]), \n",
    "               'participants':np.tile(np.arange(1,17), 4),\n",
    "              'aligned':np.concatenate([np.zeros(16), np.ones(16), np.zeros(16), np.ones(16)]), \n",
    "              'training':np.concatenate([np.zeros(16), np.zeros(16), np.ones(16), np.ones(16)])}\n",
    "isc_results['aligned_labels'] = ['pre-' if v == 0 else 'post-' for v in isc_results['aligned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c3b6af-be2d-4d70-b2e9-2d95feb2970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(context='notebook', style='white')\n",
    "g=sns.barplot(data=isc_results, x='training', \n",
    "              y='scores', hue='aligned_labels',\n",
    "              palette='mako',alpha=0.8, \n",
    "              edgecolor='k', linewidth=2)\n",
    "g=sns.stripplot(data=isc_results, x='training', y='scores', \n",
    "                dodge=True, hue='aligned_labels',\n",
    "                legend=False, palette='mako', edgecolor='k', \n",
    "                linewidth=1, )\n",
    "g.set(xticklabels=['testing','training'], ylim=[0,0.8], xlabel='', \n",
    "      ylabel='Correlation', title='ISC in training and testing sets, pre/post HA')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782003ea-5dc1-44f4-994d-5b5709bfbbd3",
   "metadata": {},
   "source": [
    "What we see here is that hyperalignment improves how synchronized participants' brain responses are with one another. That improvement is greatest in the data that were used to train the hyperalignment (`training` on the x-axis), but the transformations also generalized -- when applied to unseen data (`testing` on the x-axis), we also saw a boost in synchrony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c809d371-0aa2-43cc-8767-5dc1e9f5778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_upsample_train_data(X_train, Y_train):\n",
    "    '''\n",
    "    Randomly upsamples training samples so that there are equal samples for each class \n",
    "    Helpful for balancing classification problems\n",
    "    '''\n",
    "    (unique, counts) = np.unique(Y_train, return_counts=True)\n",
    "    goal = max(counts)\n",
    "    train_indices = []\n",
    "    for ind, classs in enumerate(unique):\n",
    "        if counts[ind] != goal:\n",
    "            num2add = goal - counts[ind]\n",
    "            where_this_class = np.where(Y_train == classs)\n",
    "            chosen_indices = list(random.choices(where_this_class[0], k=num2add)) + list(where_this_class[0])\n",
    "            train_indices = train_indices + chosen_indices\n",
    "        else:\n",
    "            train_indices += list(np.where(Y_train == classs)[0])\n",
    "    train_indices.sort()\n",
    "    X_train = X_train[train_indices]\n",
    "    Y_train = Y_train[train_indices]\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e7321-ae21-4e10-95ad-8972f64b0f83",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=['fold','aligned','accuracy'])\n",
    "label_subset = labels[n_trs//2:]\n",
    "training_labels_orig = np.tile(label_subset, 15)\n",
    "label_subset1=label_subset\n",
    "folder=LeaveOneOut()\n",
    "testing_dss_aligned=np.array(testing_dss_aligned)\n",
    "testing_dss = np.array(testing_dss)\n",
    "_, ns, nv = testing_dss.shape\n",
    "kernel='rbf'\n",
    "C=10\n",
    "# between subject classification analysis \n",
    "for i,(train_idx,test_idx) in enumerate(folder.split(np.arange(16))):\n",
    "    print(f'Fold {i}, testing on: {test_idx}')\n",
    "\n",
    "    # train on all other subjects' data; test on held-out    \n",
    "    clf_training, clf_testing = testing_dss[train_idx].reshape(len(train_idx)*ns,nv), testing_dss[test_idx].reshape(len(test_idx)*ns, nv)\n",
    "    clf_training, training_labels = random_upsample_train_data(clf_training, training_labels_orig)\n",
    "\n",
    "    clf_training_aligned, clf_testing_aligned = testing_dss_aligned[train_idx].reshape(len(train_idx)*ns,nv), testing_dss_aligned[test_idx].reshape(len(test_idx)*ns, nv)\n",
    "    clf_training_aligned, training_labels = random_upsample_train_data(clf_training_aligned, training_labels_orig)\n",
    "\n",
    "    print(f'reshaped data: {clf_training.shape}, {clf_testing.shape}, {clf_training_aligned.shape}, {clf_testing_aligned.shape}')\n",
    "    svc_orig = svm.SVC(C=C, kernel=kernel,class_weight='balanced')\n",
    "    svc_orig.fit(clf_training, training_labels)\n",
    "    acc = svc_orig.score(clf_testing, label_subset1)\n",
    "    print(f'orig={np.round(acc,3)}')\n",
    "    df.loc[len(df)] = {'fold':i, 'aligned':'pre-', 'accuracy':acc}\n",
    "\n",
    "    svc_align = svm.SVC(C=C, kernel=kernel, class_weight='balanced')\n",
    "    svc_align.fit(clf_training_aligned, training_labels)\n",
    "    acc_align = svc_align.score(clf_testing_aligned, label_subset1)\n",
    "    df.loc[len(df)] = {'fold':i, 'aligned':'post-', 'accuracy':acc_align}\n",
    "    print(f'aligned={np.round(acc_align,3)}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e6691-b266-4cc3-b8af-ec18ab34daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1,figsize=(2,4))\n",
    "g=sns.barplot(data=df, x='aligned', \n",
    "              y='accuracy', \n",
    "              order=['pre-','post-'],\n",
    "              palette='mako',alpha=0.8, \n",
    "              edgecolor='k', linewidth=2)\n",
    "\n",
    "g.set(xticklabels=['pre-','post-'], ylim=[0,1], xlabel='', \n",
    "      ylabel='accuracy', title='Between-subject classification')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c611d6-fb42-46da-87c4-1efbb9a02079",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Aligning RS-fMRI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba700dc-4706-46fb-8fb9-2488ce3baa00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded_data=load_rsfmri(20, seed_region=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c3b34-8b33-42ee-924d-e348d67c06db",
   "metadata": {},
   "outputs": [],
   "source": [
    "parcellated_data, region_data = np.array(loaded_data['parcellated_all']), np.array(loaded_data['region_all'])\n",
    "n_timepoints = region_data.shape[1]\n",
    "parcellated_split_0, region_split_0 = parcellated_data[:, :n_timepoints//2, :], region_data[:, :n_timepoints//2, :]\n",
    "parcellated_split_1, region_split_1 = parcellated_data[:, n_timepoints//2:, :], region_data[:, n_timepoints//2:, :]\n",
    "\n",
    "cnx_split0 = [np.nan_to_num(compute_connectivity_matrix(p.T, r.T)) for p,r in zip(parcellated_split_0, region_split_0)]\n",
    "cnx_split1 = [np.nan_to_num(compute_connectivity_matrix(p.T, r.T)) for p,r in zip(parcellated_split_1, region_split_1)]\n",
    "print(np.shape(cnx_split0), np.shape(cnx_split1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef46a03-cf2b-4f5a-8379-0e8bdb43741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train hyperalignment on the first half and transform the second half, and vice-versa\n",
    "model_split0 = hyp.Hyperalignment(verbose=0)\n",
    "model_split0.fit(cnx_split0)\n",
    "aligned_cnx_split1 = model_split0.apply_fit_transformations(cnx_split1)\n",
    "\n",
    "model_split1 = hyp.Hyperalignment(verbose=0)\n",
    "model_split1.fit(cnx_split1)\n",
    "aligned_cnx_split0 = model_split1.apply_fit_transformations(cnx_split0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73a2bc-a22d-4d88-994f-ab0a37e9c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate reliability of connectivity across split-halves\n",
    "def individual_differences_reliability(cnx0, cnx1):\n",
    "    pcorr0 = compute_pairwise_corr(cnx0)\n",
    "    pcorr1 = compute_pairwise_corr(cnx1)\n",
    "    return np.corrcoef(pcorr0, pcorr1)[0,1]\n",
    "   \n",
    "def compute_pairwise_corr(c):\n",
    "    n,s,t=np.array(c).shape\n",
    "    c = np.array(c).reshape(n,s*t)\n",
    "    corrs = pdist(c, 'correlation')\n",
    "    return corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c3dcc-e5e6-414c-b988-456fdac22c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running reliability of individual differences across the split halves of data, \n",
    "# before and after hyperalignment\n",
    "\n",
    "pcorr0 = squareform(compute_pairwise_corr(cnx_split0))\n",
    "pcorr1 = squareform(compute_pairwise_corr(cnx_split1))\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,4))\n",
    "r=individual_differences_reliability(cnx_split0, cnx_split1)\n",
    "g=sns.heatmap(pcorr0, ax=ax[0], vmin=0, vmax=1, cmap='viridis', square=True)\n",
    "g.set(title='Pre-HA conn ISC split 0')\n",
    "g=sns.heatmap(pcorr1, ax=ax[1], vmin=0, vmax=1, cmap='viridis', square=True)\n",
    "g.set(title=f'split 1; split-half rel ={np.round(r,3)}')\n",
    "\n",
    "\n",
    "apcorr0 = squareform(compute_pairwise_corr(aligned_cnx_split0))\n",
    "apcorr1 = squareform(compute_pairwise_corr(aligned_cnx_split1))\n",
    "r=individual_differences_reliability(aligned_cnx_split0, aligned_cnx_split1)\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,4))\n",
    "g=sns.heatmap(apcorr0, ax=ax[0], vmin=0, vmax=1, cmap='viridis', square=True)\n",
    "g.set(title='Post-HA conn ISC split 0')\n",
    "g=sns.heatmap(apcorr1, ax=ax[1], vmin=0, vmax=1, cmap='viridis', square=True)\n",
    "g.set(title=f'split 1; split-half rel ={np.round(r,3)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe5e306-0d1b-4663-82a3-5a20c0dc6187",
   "metadata": {},
   "source": [
    "# Shared response model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6408f7-1f97-44b5-bf20-780533730558",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Simulated timeseries data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95740b11-5684-4df5-bcff-bfd9ec0fad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_isc(X):\n",
    "    '''\n",
    "    Intersubject correlation analysis: \n",
    "    compares each subject's timeseries with the average of all other subjects' timeseries\n",
    "    Measure of \"synchrony\" over time across brains, which should be improved by functional alignment\n",
    "    '''\n",
    "    if len(np.shape(X)) == 2:\n",
    "        X = X[:,:,np.newaxis]\n",
    "    results = np.zeros((len(X), X[0].shape[-1]))\n",
    "    for i in range(len(X)):\n",
    "        test = X[i]\n",
    "        tr_idx = np.setdiff1d(np.arange(len(X)),i)\n",
    "        train = np.mean(np.array([X[j] for j in tr_idx]), axis=0)\n",
    "        # get correlation at each feature\n",
    "        cmat = cdist(test.T, train.T, 'correlation')\n",
    "        corrs = [1-cmat[i,i] for i in range(cmat.shape[0])]\n",
    "        results[i]= corrs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd1e0e-7d3d-45e1-80cd-cb12364b0b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets with the following parameters\n",
    "n_participants, n_samples, n_features = 12, 100, 20\n",
    "within_corr = 0.8\n",
    "between_corr = 0.2\n",
    "noise=0.1\n",
    "\n",
    "simulator = SimulatedCorrelatedData(n_datasets=n_participants, \n",
    "                        n_samples=n_samples, \n",
    "                        n_features_measured=n_features,\n",
    "                        n_features_latent=2,\n",
    "                       noise_sigma=noise,\n",
    "                       intra_ds_corr=within_corr,\n",
    "                       inter_ds_corr=between_corr,\n",
    "                       verbose=0)\n",
    "dss = simulator.generate()\n",
    "\n",
    "# Fit model with 5 features\n",
    "srm_model = srm.SRM(features=5)\n",
    "\n",
    "# Have to reshape to be [subjects, features, samples] instead\n",
    "srm_model.fit([d.T for d in dss])\n",
    "\n",
    "# Transform the data\n",
    "srm_data = srm_model.transform([d.T for d in dss])\n",
    "srm_data = [d.T for d in srm_data]\n",
    "iscs_srmed = np.mean(run_isc(srm_data),axis=1)\n",
    "iscs_pre = np.mean(run_isc(dss),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647eed29-58df-4182-a1be-3c1610a8c4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a270529-b96d-4f0b-a2f6-d24368efdb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1,figsize=(2,4))\n",
    "\n",
    "g=sns.barplot([iscs_pre, iscs_srmed], palette='mako', ec='k')\n",
    "g.set(xticklabels=['pre-srm','post-srm'], ylabel='correlation')\n",
    "g=sns.stripplot([iscs_pre, iscs_srmed], palette='mako', dodge=False, edgecolor='k', \n",
    "                linewidth=1,)\n",
    "g.set(xticklabels=['pre-srm','post-srm'], ylabel='correlation', title='ISC, simulated data')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26420658-7add-4a30-92b3-de326d6363fa",
   "metadata": {},
   "source": [
    "## Movie timeseries data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce4d894-34cc-467e-9a92-0f3595347386",
   "metadata": {},
   "outputs": [],
   "source": [
    "dss = load_sherlock_movie()\n",
    "labels = load_sherlock_labels()\n",
    "n_participants, n_trs, n_voxels = dss.shape # 16 participants ,1976 timepoints, 307 voxels\n",
    "print(dss.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d259f-5b6c-4c9e-94eb-1516eeba32ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dss = stats.zscore(dss[:, :n_trs//2, :],axis=1) # training on first half,zscore across timepoints\n",
    "testing_dss = stats.zscore(dss[:, n_trs//2:, :],axis=1) # testing on second half\n",
    "srm_model = srm.SRM(features=50)\n",
    "srm_model.fit([d.T for d in training_dss])\n",
    "\n",
    "# Transform the training data\n",
    "srm_training_dss = srm_model.transform([d.T for d in training_dss])\n",
    "srm_training_dss = np.array([d.T for d in srm_training_dss])\n",
    "\n",
    "# Transform the testing data\n",
    "srm_testing_dss = srm_model.transform([d.T for d in testing_dss])\n",
    "srm_testing_dss = np.array([d.T for d in srm_testing_dss])\n",
    "\n",
    "# Evaluate intersubject correlations\n",
    "training_isc = run_isc(training_dss)\n",
    "testing_isc = run_isc(testing_dss)\n",
    "srm_training_isc = run_isc(srm_training_dss)\n",
    "srm_testing_isc = run_isc(srm_testing_dss)\n",
    "\n",
    "# Average results to get one per subject per split\n",
    "r0 = np.mean(training_isc,axis=1)\n",
    "r1 = np.mean(testing_isc, axis=1)\n",
    "r2 = np.mean(srm_training_isc, axis=1)\n",
    "r3 = np.mean(srm_testing_isc, axis=1)\n",
    "isc_results = {'scores': np.concatenate([r0,r1,r2,r3]), \n",
    "               'participants':np.tile(np.arange(1,17), 4),\n",
    "              'aligned':np.concatenate([np.zeros(16), np.zeros(16), np.ones(16), np.ones(16)]), \n",
    "              'training':np.concatenate([np.ones(16), np.zeros(16), np.ones(16), np.zeros(16)])}\n",
    "isc_results['aligned_labels'] = ['pre-' if v == 0 else 'post-' for v in isc_results['aligned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97053737-9a37-4394-853c-f491b9aabd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(context='notebook', style='white')\n",
    "g=sns.barplot(data=pd.DataFrame(isc_results), x='training', \n",
    "              y='scores', hue='aligned_labels',\n",
    "              palette='mako',alpha=0.8, \n",
    "              edgecolor='k', linewidth=2)\n",
    "g=sns.stripplot(data=isc_results, x='training', y='scores', \n",
    "                dodge=True, hue='aligned_labels',\n",
    "                legend=False, palette='mako', edgecolor='k', \n",
    "                linewidth=1, )\n",
    "g.set(xticks=[0,1],xticklabels=['testing','training'], ylim=[0,0.8], xlabel='', \n",
    "      ylabel='Correlation', title='ISC in training and testing sets, pre/post SRM')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250ee44-6937-444e-acc9-ed64cc56bc60",
   "metadata": {},
   "source": [
    "Something else we can do with SRM is tune the number of shared responses learned, with cross-validation. This is how it would work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f6026-2834-4679-ac04-d6d28a0f5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through feature hyperparameter and compute ISC on the training data\n",
    "K_values = np.arange(5, 100, 5)\n",
    "average_iscs = np.zeros(len(K_values))\n",
    "for i,k in enumerate(K_values):\n",
    "    srm_model = srm.SRM(features=k)\n",
    "    srm_model.fit([d.T for d in training_dss])\n",
    "    # now score ISC on the training data\n",
    "    dss_srm = srm_model.transform([d.T for d in training_dss])\n",
    "    corr = np.mean(run_isc(dss_srm))\n",
    "    average_iscs[i] = corr\n",
    "\n",
    "# Choose the K that resulted in the highest isc\n",
    "maxx = np.argmax(average_iscs)\n",
    "best_k = K_values[maxx]\n",
    "srm_model = srm.SRM(features=k)\n",
    "srm_model.fit([d.T for d in training_dss])\n",
    "srm_training_dss = srm_model.transform([d.T for d in training_dss])\n",
    "srm_testing_dss = srm_model.transform([d.T for d in testing_dss])\n",
    "\n",
    "# Evaluate intersubject correlations\n",
    "training_isc = run_isc(training_dss)\n",
    "testing_isc = run_isc(testing_dss)\n",
    "srm_training_isc = run_isc(srm_training_dss)\n",
    "srm_testing_isc = run_isc(srm_testing_dss)\n",
    "\n",
    "# Average results to get one per subject per split\n",
    "r0 = np.mean(training_isc,axis=1)\n",
    "r1 = np.mean(testing_isc, axis=1)\n",
    "r2 = np.mean(srm_training_isc, axis=1)\n",
    "r3 = np.mean(srm_testing_isc, axis=1)\n",
    "isc_results = {'scores': np.concatenate([r0,r1,r2,r3]), \n",
    "               'participants':np.tile(np.arange(1,17), 4),\n",
    "              'aligned':np.concatenate([np.zeros(16), np.zeros(16), np.ones(16), np.ones(16)]), \n",
    "              'training':np.concatenate([np.ones(16), np.zeros(16), np.ones(16), np.zeros(16)])}\n",
    "isc_results['aligned_labels'] = ['pre-' if v == 0 else 'post-' for v in isc_results['aligned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad65ed37-3d0d-4dec-8f78-385d3e086d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(context='notebook', style='white')\n",
    "g=sns.barplot(data=isc_results, x='training', \n",
    "              y='scores', hue='aligned_labels',\n",
    "              palette='mako',alpha=0.8, \n",
    "              edgecolor='k', linewidth=2)\n",
    "g=sns.stripplot(data=isc_results, x='training', y='scores', \n",
    "                dodge=True, hue='aligned_labels',\n",
    "                legend=False, palette='mako', edgecolor='k', \n",
    "                linewidth=1, )\n",
    "g.set(xticks=[0,1],xticklabels=['testing','training'], ylim=[0,0.8], xlabel='', \n",
    "      ylabel='Correlation', title=f'ISC in training and testing sets, pre/post SRM with K={best_k} tuned')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ee945-1323-4c7c-8cc1-4d3d1507689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.DataFrame(columns=['fold','aligned','accuracy'])\n",
    "label_subset = labels[n_trs//2:]\n",
    "training_labels_orig = np.tile(label_subset, 15)\n",
    "label_subset1=label_subset#np.tile(label_subset, 4)\n",
    "folder=LeaveOneOut()#KFold(n_splits=4)\n",
    "testing_dss_aligned=np.array(testing_dss_aligned)\n",
    "testing_dss = np.array(testing_dss)\n",
    "_, ns, nv = testing_dss.shape\n",
    "kernel='rbf'\n",
    "C=10\n",
    "# between subject classification analysis \n",
    "for i,(train_idx,test_idx) in enumerate(folder.split(np.arange(16))):\n",
    "    print(f'Fold {i}, testing on: {test_idx}')\n",
    "    \n",
    "    clf_training_aligned, clf_testing_aligned = testing_dss_aligned[train_idx].reshape(len(train_idx)*ns,nv), testing_dss_aligned[test_idx].reshape(len(test_idx)*ns, nv)\n",
    "    clf_training_aligned, training_labels = random_upsample_train_data(clf_training_aligned, training_labels_orig)\n",
    "\n",
    "    svc_align = svm.SVC(C=C, kernel=kernel, class_weight='balanced')\n",
    "    svc_align.fit(clf_training_aligned, training_labels)\n",
    "    acc_align = svc_align.score(clf_testing_aligned, label_subset1)\n",
    "    df1.loc[len(df1)] = {'fold':i, 'aligned':'post-', 'accuracy':acc_align}\n",
    "    print(f'aligned={np.round(acc_align,3)}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98415ff-1b2e-4177-990b-40ce2dfe2fb1",
   "metadata": {},
   "source": [
    "## Aligning RS-fMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e4015-8103-4edb-bc3b-38e2e9642209",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data=load_rsfmri(20, seed_region=10)\n",
    "parcellated_data, region_data = np.array(loaded_data['parcellated_all']), np.array(loaded_data['region_all'])\n",
    "n_timepoints = region_data.shape[1]\n",
    "parcellated_split_0, region_split_0 = parcellated_data[:, :n_timepoints//2, :], region_data[:, :n_timepoints//2, :]\n",
    "parcellated_split_1, region_split_1 = parcellated_data[:, n_timepoints//2:, :], region_data[:, n_timepoints//2:, :]\n",
    "\n",
    "cnx_split0 = [np.nan_to_num(compute_connectivity_matrix(p.T, r.T)) for p,r in zip(parcellated_split_0, region_split_0)]\n",
    "cnx_split1 = [np.nan_to_num(compute_connectivity_matrix(p.T, r.T)) for p,r in zip(parcellated_split_1, region_split_1)]\n",
    "print(np.shape(cnx_split0), np.shape(cnx_split1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b18c1f-ad8e-4a96-a850-e894365295ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SRM on the first half and transform the second half, and vice-versa\n",
    "\n",
    "cnx_split1T = np.array([c for c in cnx_split1])\n",
    "cnx_split0T = np.array([c for c in cnx_split0]) # reshape \n",
    "\n",
    "model_split0 = srm.SRM(features=20)\n",
    "model_split0.fit(cnx_split0T)\n",
    "\n",
    "aligned_cnx_split1 = model_split0.transform(cnx_split1T)\n",
    "aligned_cnx_split1 = np.array([s for s in aligned_cnx_split1])\n",
    "\n",
    "\n",
    "model_split1 = srm.SRM(features=20)\n",
    "model_split1.fit(cnx_split1T)\n",
    "\n",
    "aligned_cnx_split0 = model_split1.transform(cnx_split0T)\n",
    "aligned_cnx_split0 = np.array([s for s in aligned_cnx_split0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e7879a-d32b-45ab-ae09-456637f8ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate reliability of connectivity across split-halves\n",
    "def individual_differences_reliability(cnx0, cnx1):\n",
    "    pcorr0 = compute_pairwise_corr(cnx0)\n",
    "    pcorr1 = compute_pairwise_corr(cnx1)\n",
    "    return np.corrcoef(pcorr0, pcorr1)[0,1]\n",
    "   \n",
    "def compute_pairwise_corr(c):\n",
    "    n,s,t=np.array(c).shape\n",
    "    c = np.array(c).reshape(n,s*t)\n",
    "    corrs = pdist(c, 'correlation')\n",
    "    return corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ad9e1-c2ad-4094-bb97-ccb1d65c73f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running reliability of individual differences across the split halves of data, \n",
    "# before and after hyperalignment\n",
    "\n",
    "pcorr0 = squareform(compute_pairwise_corr(cnx_split0))\n",
    "pcorr1 = squareform(compute_pairwise_corr(cnx_split1))\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,4))\n",
    "r=individual_differences_reliability(cnx_split0, cnx_split1)\n",
    "g=sns.heatmap(pcorr0, ax=ax[0], vmin=0, vmax=1.5, cmap='viridis', square=True)\n",
    "g.set(title='Pre-SRM corr. diff. split 0')\n",
    "g=sns.heatmap(pcorr1, ax=ax[1], vmin=0, vmax=1.5, cmap='viridis', square=True)\n",
    "g.set(title=f'Pre-SRM corr. diff. split 1; rel = {np.round(r,3)}')\n",
    "\n",
    "\n",
    "apcorr0 = squareform(compute_pairwise_corr(aligned_cnx_split0))\n",
    "apcorr1 = squareform(compute_pairwise_corr(aligned_cnx_split1))\n",
    "r=individual_differences_reliability(aligned_cnx_split0, aligned_cnx_split1)\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,4))\n",
    "g=sns.heatmap(apcorr0, ax=ax[0], vmin=0, vmax=1.5, cmap='viridis', square=True)\n",
    "g.set(title=f'Post-SRM corr. diff. split 0')\n",
    "g=sns.heatmap(apcorr1, ax=ax[1], vmin=0, vmax=1.5, cmap='viridis', square=True)\n",
    "g.set(title=f'Post-SRM corr. diff. split 1; rel = {np.round(r,3)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b86a3-3f17-42d2-aed4-6f6be8ba3296",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
